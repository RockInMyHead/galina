import Header from "@/components/Header";
import { Button } from "@/components/ui/button";
import { Card, CardContent } from "@/components/ui/card";
import { Mic, MicOff, Sparkles, Send, Download, FileText } from "lucide-react";
import { useState, useEffect, useRef, useCallback } from "react";
import { sendChatMessage, textToSpeech, playAudioBlob, speechToText } from "@/utils/apiUtils";
import { AI_SYSTEM_MESSAGES, API_CONFIG } from "@/config/constants";
import jsPDF from 'jspdf';

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
}

const Voice = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  const [showTestMode, setShowTestMode] = useState(false);
  
  // Auto-send timer
  const autoSendTimerRef = useRef<NodeJS.Timeout | null>(null);
  const SILENCE_TIMEOUT = 2000; // 2 seconds
  const [autoSendStatus, setAutoSendStatus] = useState<'idle' | 'waiting' | 'sending'>('idle');

  // Environment detection
  const isSecure = window.isSecureContext || location.protocol === 'https:';

  // Audio recording refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  // Messages state
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      content: '–ü—Ä–∏–≤–µ—Ç! –Ø –ì–∞–ª–∏–Ω–∞, –≤–∞—à AI-—é—Ä–∏—Å—Ç. –Ø –≥–æ—Ç–æ–≤–∞ –ø–æ–º–æ—á—å –≤–∞–º —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å –≥–æ–ª–æ—Å–æ–º –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º.',
      role: 'assistant',
      timestamp: new Date()
    }
  ]);

  // Loading states for TTS
  const [isGeneratingTTS, setIsGeneratingTTS] = useState(false);
  const [isPlayingTTS, setIsPlayingTTS] = useState(false);

  // Auto-send status
  const [isAutoSending, setIsAutoSending] = useState(false);

  // Conversation summary
  const [conversationSummary, setConversationSummary] = useState<string[]>([]);
  const [isGeneratingSummary, setIsGeneratingSummary] = useState(false);

  // Detailed logging of environment detection
  useEffect(() => {
    console.log('üîç Environment detection:', {
      hostname: location.hostname,
      protocol: location.protocol,
      port: location.port,
      href: location.href,
      isSecure,
      secureContext: window.isSecureContext,
      userAgent: navigator.userAgent.substring(0, 50) + '...'
    });

      console.log('‚ÑπÔ∏è Environment check:', {
        isSecure,
      reason: isSecure ? 'secure context' : 'insecure context - may have voice issues'
    });
  }, [isSecure]);

  // Initialize audio recording capabilities
  useEffect(() => {
    console.log('üîß Checking audio recording capabilities...');
    console.log('üìä Browser capabilities:', {
      mediaDevices: !!navigator.mediaDevices,
      getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
      mediaRecorder: !!window.MediaRecorder
    });

    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      console.error('‚ùå getUserMedia not supported - cannot record audio');
      return;
    }

    if (!window.MediaRecorder) {
      console.error('‚ùå MediaRecorder not supported - cannot record audio');
      return;
    }

    console.log('‚úÖ Audio recording supported - ready to use Whisper API');

    // Cleanup function
    return () => {
      if (autoSendTimerRef.current) {
        clearTimeout(autoSendTimerRef.current);
        console.log('üßπ Cleaned up auto-send timer on unmount');
      }
    };
  }, []);

  // Beep functionality for user feedback
  const playBeep = useCallback(() => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);

      oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
      gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.1);

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.1);

      // Close context after beep
      setTimeout(() => audioContext.close(), 200);
    } catch (error) {
      console.warn('Could not play beep:', error);
    }
  }, []);

  const beepIntervalRef = useRef<NodeJS.Timeout | null>(null);

  const startBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) return; // Already beeping

    console.log('üîä Starting beep interval for AI processing feedback');
    playBeep(); // Play first beep immediately
    beepIntervalRef.current = setInterval(playBeep, 3000); // Every 3 seconds
  }, [playBeep]);

  const stopBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current);
      beepIntervalRef.current = null;
      console.log('üîá Stopped beep interval');
    }
  }, []);

  // Manage beep interval based on app state
  useEffect(() => {
    // Start beep when loading (AI thinking) or when TTS is being generated, but NOT during TTS playback
    if ((isProcessingAudio || isGeneratingTTS) && !isPlayingTTS) {
      startBeepInterval();
    } else {
      stopBeepInterval();
    }

    // Cleanup on unmount
    return () => {
      stopBeepInterval();
    };
  }, [isProcessingAudio, isGeneratingTTS, isPlayingTTS, startBeepInterval, stopBeepInterval]);

  // Generate conversation summary
  const generateConversationSummary = useCallback(async (messages: Message[]) => {
    if (messages.length < 2) return; // Need at least user question and AI response

    setIsGeneratingSummary(true);

    try {
      // Extract key points from conversation
      const userMessages = messages.filter(m => m.role === 'user').map(m => m.content);
      const aiMessages = messages.filter(m => m.role === 'assistant').slice(1); // Skip greeting

      const summary = [];

      // Add main topics/questions
      if (userMessages.length > 0) {
        summary.push(`üìã –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–∞:`);
        userMessages.forEach((msg, index) => {
          const shortMsg = msg.length > 100 ? msg.substring(0, 100) + '...' : msg;
          summary.push(`   ${index + 1}. ${shortMsg}`);
        });
      }

      // Add key recommendations from AI
      if (aiMessages.length > 0) {
        summary.push(``);
        summary.push(`üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —é—Ä–∏—Å—Ç–∞:`);

        aiMessages.forEach((msg, index) => {
          // Extract key sentences that contain advice
          const sentences = msg.content.split(/[.!?]+/).filter(s => s.trim().length > 10);

          sentences.forEach(sentence => {
            const trimmed = sentence.trim();
            // Look for sentences with advice keywords
            if (trimmed.includes('—Ä–µ–∫–æ–º–µ–Ω–¥—É—é') ||
                trimmed.includes('—Å–ª–µ–¥—É–µ—Ç') ||
                trimmed.includes('–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ') ||
                trimmed.includes('–≤–∞–∂–Ω–æ') ||
                trimmed.includes('–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å') ||
                trimmed.includes('–ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ') ||
                trimmed.includes('—Å–æ—Å—Ç–∞–≤—å—Ç–µ')) {
              summary.push(`   ‚Ä¢ ${trimmed}`);
            }
          });

          // If no specific advice found, add a general summary
          if (summary.length === 2) { // Only header added
            const shortResponse = msg.content.length > 150 ? msg.content.substring(0, 150) + '...' : msg.content;
            summary.push(`   ‚Ä¢ ${shortResponse}`);
          }
        });
      }

      // Add conversation metadata
      summary.push(``);
      summary.push(`üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏:`);
      summary.push(`   ‚Ä¢ –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: ${new Date().toLocaleString('ru-RU')}`);
      summary.push(`   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π: ${messages.length}`);
      summary.push(`   ‚Ä¢ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~${Math.ceil(messages.length / 2)} –º–∏–Ω`);

      setConversationSummary(summary);
    } catch (error) {
      console.error('Error generating summary:', error);
      setConversationSummary(['–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞']);
    } finally {
      setIsGeneratingSummary(false);
    }
  }, []);

  // Generate PDF from conversation summary
  const downloadConversationPDF = useCallback(async () => {
    if (conversationSummary.length === 0) return;

    try {
      const pdf = new jsPDF();
      const pageWidth = pdf.internal.pageSize.getWidth();
      const pageHeight = pdf.internal.pageSize.getHeight();
      const margin = 20;
      let yPosition = 30;

      // Title
      pdf.setFontSize(18);
      pdf.setFont('helvetica', 'bold');
      pdf.text('–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è - –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã', margin, yPosition);
      yPosition += 20;

      // Date
      pdf.setFontSize(12);
      pdf.setFont('helvetica', 'normal');
      pdf.text(`–î–∞—Ç–∞: ${new Date().toLocaleDateString('ru-RU')}`, margin, yPosition);
      yPosition += 15;

      // Content
      pdf.setFontSize(11);
      conversationSummary.forEach((line) => {
        if (yPosition > pageHeight - 30) {
          pdf.addPage();
          yPosition = 30;
        }

        // Handle different line types
        if (line.includes('üìã') || line.includes('üí°') || line.includes('üìä')) {
          pdf.setFont('helvetica', 'bold');
          pdf.text(line, margin, yPosition);
            } else {
          pdf.setFont('helvetica', 'normal');
          pdf.text(line, margin, yPosition);
        }

        yPosition += 8;
      });

      // Footer
      pdf.setFontSize(8);
      pdf.setFont('helvetica', 'italic');
      pdf.text('–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ AI-—é—Ä–∏—Å—Ç–æ–º –ì–∞–ª–∏–Ω–æ–π', margin, pageHeight - 20);

      // Download
      const fileName = `consultation-summary-${new Date().toISOString().split('T')[0]}.pdf`;
      pdf.save(fileName);

      console.log('‚úÖ PDF generated and downloaded:', fileName);
    } catch (error) {
      console.error('‚ùå Error generating PDF:', error);
      alert('–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF —Ñ–∞–π–ª–∞');
    }
  }, [conversationSummary]);

  // Handle sending messages to AI
  const handleSendMessage = useCallback(async (messageText?: string) => {
    const textToSend = messageText || transcript;
    if (!textToSend.trim()) return;

    console.log('üöÄ handleSendMessage called with:', textToSend);

    // Clear auto-send timer if running
      if (autoSendTimerRef.current) {
        clearTimeout(autoSendTimerRef.current);
      console.log('üïê Cleared auto-send timer on send');
    }

    // Add user message to chat
    const userMessage: Message = {
      id: Date.now().toString(),
      content: textToSend,
      role: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setTranscript('');
    setInterimTranscript('');
    setAutoSendStatus('idle');

    try {
      // Send to AI
      const response = await sendChatMessage([{
        role: 'user',
        content: textToSend
      }]);

      if (response.success && response.data.content) {
        const aiMessage: Message = {
          id: (Date.now() + 1).toString(),
          content: response.data.content,
          role: 'assistant',
          timestamp: new Date()
        };

        const updatedMessages = [...messages, userMessage, aiMessage];
        setMessages(updatedMessages);

        // Generate conversation summary after AI response
        setTimeout(() => {
          generateConversationSummary(updatedMessages);
        }, 1000); // Delay to allow UI to update

        // Auto-generate TTS for AI response
        speakAIResponse(response.data.content);
    } else {
        console.error('‚ùå AI response error:', response.error);
        const errorMessage: Message = {
          id: (Date.now() + 1).toString(),
          content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.',
          role: 'assistant',
          timestamp: new Date()
        };
        setMessages(prev => [...prev, errorMessage]);
      }
    } catch (error) {
      console.error('‚ùå Error in handleSendMessage:', error);
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.',
        role: 'assistant',
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    }
  }, [transcript]);

  // TTS function for AI responses using OpenAI with parallel generation
  const speakAIResponse = useCallback(async (responseText: string) => {
    if (!responseText || !isSecure) return;

      console.log('üéµ Preparing parallel OpenAI TTS for AI response...');
      setIsGeneratingTTS(true);

    try {
      // Split response into sentences for parallel processing
      const sentences = responseText.split(/[.!?]+/).filter(s => s.trim().length > 0);
      console.log('üìù Split into', sentences.length, 'sentences for parallel TTS');

      // Process and generate TTS for each sentence in parallel
      const ttsPromises = sentences.map(async (sentence, index) => {
        const cleanSentence = sentence.trim();
        if (!cleanSentence) return null;

        console.log(`üéµ Generating TTS for sentence ${index + 1}/${sentences.length}: "${cleanSentence.substring(0, 50)}..."`);

        try {
          const audioBlob = await textToSpeech(cleanSentence);
          return { audio: audioBlob, text: cleanSentence, index };
    } catch (error) {
          console.error(`‚ùå Failed to generate TTS for sentence ${index + 1}:`, error);
          return null;
        }
      });

      // Wait for all TTS generations to complete
      const results = await Promise.allSettled(ttsPromises);
      console.log('‚è≥ Waiting for all parallel TTS generations...');

      // Check if any TTS generation failed
      const failedGenerations = results.filter(result => result.status === 'rejected').length;
      if (failedGenerations > 0) {
        console.warn(`‚ö†Ô∏è ${failedGenerations} TTS generations failed`);
      }

      console.log('‚ñ∂Ô∏è Starting sequential playback...');
      console.log('üé¨ VIDEO SHOULD APPEAR NOW - setIsPlayingTTS(true)');
      setIsPlayingTTS(true);

      let ttsFailed = false;
      for (const result of results) {
        if (result.status === 'fulfilled' && result.value?.audio) {
          const { audio, index } = result.value;
          console.log(`üéµ Playing sentence ${index + 1}, size: ${audio.size} bytes`);
          console.log(`üîä AUDIO SHOULD PLAY NOW for sentence ${index + 1}`);
          const playbackSuccess = await playAudioBlob(audio);
          console.log(`‚úÖ Finished playing sentence ${index + 1}, success: ${playbackSuccess}`);

          if (!playbackSuccess) {
            ttsFailed = true;
          }

          // Small pause between sentences
          if (index < results.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 300));
          }
        }
      }

      // Show notification if TTS failed
      if (ttsFailed) {
        console.log('‚ö†Ô∏è TTS playback failed, showing notification to user');
        // You could add a toast notification here if desired
      }

      setIsPlayingTTS(false);
      console.log('üé¨ VIDEO SHOULD DISAPPEAR NOW - setIsPlayingTTS(false)');

      console.log('‚úÖ Parallel TTS completed for all sentences');
      console.log('‚úÖ TTS function completed - AUDIO SHOULD BE PLAYING');

    } catch (error) {
      console.error('‚ùå TTS function error:', error);
      setIsPlayingTTS(false);
    } finally {
      setIsGeneratingTTS(false);
    }
  }, [isSecure]);

  // Voice interaction handler - simplified for auto-send workflow
  const handleVoiceInteraction = async () => {
    console.log('handleVoiceInteraction called:', {
      isRecording,
      hasTranscript: !!transcript.trim(),
      isProcessing: isProcessingAudio
    });

    if (isRecording) {
      console.log('Stopping current recording');
      stopListening();
    } else if (!isProcessingAudio) {
      console.log('Starting new recording (will auto-send to LLM after transcription)');
      await startListening();
    } else {
      console.log('Processing audio, please wait...');
    }
  };

  // Start listening function using Whisper API
  const startListening = useCallback(async () => {
    console.log('üéØ startListening called with Whisper API, current state:', {
      isRecording,
      continuousListening: isContinuousListening
    });

    if (isRecording) {
      console.log('‚ö†Ô∏è Already recording, ignoring start request');
      return;
    }

    try {
      console.log('‚ñ∂Ô∏è Starting audio recording process...');

      // Check microphone permissions
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
          console.error('‚ùå getUserMedia not supported');
          alert('–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
          return;
        }

      console.log('üéôÔ∏è Requesting microphone permission...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000 // Good for Whisper
        }
      });

      console.log('‚úÖ Microphone permission granted');

      // Check if we're in a secure context
      if (!isSecure) {
        console.log('‚ö†Ô∏è Not in secure context - audio recording may not work');
        alert('–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç—Å—è HTTPS —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.');
        stream.getTracks().forEach(track => track.stop());
        return;
      }

      // Create MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus' // Better for Whisper
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        console.log('üéµ Audio recording stopped, processing...');
        setIsProcessingAudio(true);

        try {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
          console.log('üì¶ Audio blob created, size:', audioBlob.size, 'bytes');

          if (audioBlob.size < 1000) {
            console.warn('‚ö†Ô∏è Audio blob too small, likely empty recording');
            setTranscript('–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –∞—É–¥–∏–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
            setIsProcessingAudio(false);
            return;
          }

          // Send to Whisper API
          const transcription = await speechToText(audioBlob);
          console.log('‚úÖ Transcription received:', transcription);

          setTranscript(transcription);

          // Auto-send transcription to LLM immediately after successful transcription
          if (transcription.trim()) {
            console.log('üöÄ Auto-sending transcription to LLM...');
            setIsAutoSending(true);

            // Small delay to let user see the transcription before sending
            setTimeout(async () => {
              try {
                await handleSendMessage(transcription);
              } finally {
                setIsAutoSending(false);
              }
            }, 800); // 800ms delay to show the status
          }

    } catch (error) {
          console.error('‚ùå Transcription error:', error);
          setTranscript('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
        } finally {
          setIsProcessingAudio(false);
        }

        // Clean up
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.onerror = (event) => {
        console.error('‚ùå MediaRecorder error:', event);
        setTranscript('–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.');
        setIsRecording(false);
        setIsProcessingAudio(false);
        stream.getTracks().forEach(track => track.stop());
      };

      console.log('üé¨ Starting audio recording...');
      setIsRecording(true);
      mediaRecorder.start();

      // Auto-stop after 30 seconds if still recording
      setTimeout(() => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
          console.log('‚è∞ Auto-stopping recording after 30 seconds');
      stopListening();
        }
      }, 30000);

    } catch (error) {
      console.error('‚ùå Error starting audio recording:', error);
      if (error.name === 'NotAllowedError') {
        alert('–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.');
    } else {
        alert('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ' + error.message);
      }
    }
  }, [isRecording, isContinuousListening, isSecure]);

  // Stop listening function
  const stopListening = useCallback(() => {
    console.log('üõë Stopping audio recording');
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    setIsRecording(false);
    setIsContinuousListening(false);
    setInterimTranscript('');
  }, []);

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100">
      <Header />
      
      <div className="container mx-auto px-4 py-8">
        <div className="max-w-4xl mx-auto">
          <div className="text-center mb-8">
            <h1 className="text-4xl font-bold text-gray-900 mb-4">
              –ì–æ–ª–æ—Å–æ–≤–æ–π AI-–Æ—Ä–∏—Å—Ç –ì–∞–ª–∏–Ω–∞
            </h1>
            <p className="text-xl text-gray-600">
              –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –≥–æ–ª–æ—Å–æ–º - –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é
            </p>
          </div>

          {/* Chat Messages */}
          <Card className="mb-6">
            <CardContent className="p-6">
              <div className="space-y-4 max-h-96 overflow-y-auto">
                {messages.map((message) => (
                  <div
                    key={message.id}
                    className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
                  >
                    <div
                      className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
                        message.role === 'user'
                          ? 'bg-blue-500 text-white'
                          : 'bg-gray-200 text-gray-800'
                      }`}
                    >
                      <p className="text-sm">{message.content}</p>
                      <p className="text-xs opacity-70 mt-1">
                        {message.timestamp.toLocaleTimeString()}
                      </p>
                    </div>
                      </div>
                ))}
                      </div>
            </CardContent>
          </Card>

          {/* Voice Controls */}
          <Card className="mb-6">
            <CardContent className="p-6">
              <div className="text-center">
                <Button
                  onClick={handleVoiceInteraction}
                  disabled={isProcessingAudio || isAutoSending}
                  size="lg"
                  className={`mb-4 ${
                    isRecording
                      ? 'bg-red-500 hover:bg-red-600 animate-pulse'
                      : isProcessingAudio
                      ? 'bg-orange-500 hover:bg-orange-600'
                      : isAutoSending
                      ? 'bg-purple-500 hover:bg-purple-600'
                      : 'bg-blue-500 hover:bg-blue-600'
                  }`}
                >
                  {isAutoSending ? (
                    <>
                      <Sparkles className="mr-2 h-5 w-5 animate-spin" />
                      –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ AI...
                    </>
                  ) : isProcessingAudio ? (
                    <>
                      <Sparkles className="mr-2 h-5 w-5 animate-spin" />
                      –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏...
                    </>
                  ) : isRecording ? (
                    <>
                      <MicOff className="mr-2 h-5 w-5" />
                      –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å
                    </>
                  ) : (
                    <>
                      <Mic className="mr-2 h-5 w-5" />
                      –ù–∞—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é
                    </>
                  )}
                </Button>

                {isProcessingAudio && (
                  <div className="text-center mb-4">
                    <div className="inline-flex items-center">
                      <Sparkles className="mr-2 h-5 w-5 animate-spin text-blue-500" />
                      <span className="text-blue-600 font-medium">
                        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é Whisper...
                      </span>
                      </div>
                    </div>
                  )}

                        {transcript && (
                  <div className="bg-blue-50 rounded-lg p-4 mb-4">
                    <p className="text-blue-800 font-medium mb-2">–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:</p>
                    <p className="text-blue-700">{transcript}</p>
                  </div>
                )}

                        {interimTranscript && (
                  <div className="bg-yellow-50 rounded-lg p-4 mb-4">
                    <p className="text-yellow-800 font-medium mb-2">–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:</p>
                    <p className="text-yellow-700 italic">{interimTranscript}</p>
                          </div>
                        )}

                {/* Status indicators */}
                <div className="flex justify-center space-x-4 text-sm text-gray-600 mb-4">
                  <span className={isSecure ? 'text-green-600' : 'text-red-600'}>
                    üîí {isSecure ? '–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç' : '–ù–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç'}
                  </span>
                  <span className={isRecording ? 'text-red-600' : 'text-green-600'}>
                    üéôÔ∏è {isRecording ? '–ó–∞–ø–∏—Å—å –∞–∫—Ç–∏–≤–Ω–∞' : '–ì–æ—Ç–æ–≤ –∫ –∑–∞–ø–∏—Å–∏'}
                  </span>
                  <span className={isProcessingAudio ? 'text-blue-600' : 'text-gray-400'}>
                    ü§ñ {isProcessingAudio ? '–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏' : '–û–∂–∏–¥–∞–Ω–∏–µ'}
                  </span>
                  {isAutoSending && (
                    <span className="text-orange-600 animate-pulse">
                      üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ AI...
                    </span>
                        )}
                      </div>

                {/* Auto-send info */}
                <div className="text-center text-sm text-gray-500 mb-4">
                  üéØ –ü–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏ —Ç–µ–∫—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—Å—è –≤ AI –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
                      </div>
                    </div>
            </CardContent>
          </Card>

          {/* Conversation Summary */}
          {conversationSummary.length > 0 && (
            <Card className="mb-6">
              <CardContent className="p-6">
                <div className="flex items-center justify-between mb-4">
                  <div className="flex items-center">
                    <FileText className="mr-2 h-5 w-5 text-blue-500" />
                    <h3 className="text-lg font-semibold text-gray-800">
                      –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
                    </h3>
                  </div>
                          <Button
                    onClick={downloadConversationPDF}
                            size="sm"
                            variant="outline"
                    className="flex items-center"
                    disabled={isGeneratingSummary}
                  >
                    <Download className="mr-2 h-4 w-4" />
                    –°–∫–∞—á–∞—Ç—å PDF
                          </Button>
                      </div>

                {isGeneratingSummary ? (
                  <div className="text-center py-4">
                    <Sparkles className="mx-auto h-6 w-6 animate-spin text-blue-500 mb-2" />
                    <p className="text-gray-600">–§–æ—Ä–º–∏—Ä—É—é —Å–≤–æ–¥–∫—É —Ä–∞–∑–≥–æ–≤–æ—Ä–∞...</p>
                  </div>
                ) : (
                  <div className="bg-gray-50 rounded-lg p-4 max-h-64 overflow-y-auto">
                    <div className="text-sm text-gray-700 space-y-1 font-mono">
                      {conversationSummary.map((line, index) => (
                        <div key={index} className={line.trim() === '' ? 'h-2' : ''}>
                          {line.trim() === '' ? '\u00A0' : line}
                        </div>
                      ))}
                      </div>
                    </div>
                )}
              </CardContent>
            </Card>
          )}

          {/* Text Input Fallback */}
          <Card>
            <CardContent className="p-6">
              <div className="text-center">
                <p className="text-gray-600 mb-4">
                  –ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º:
                </p>
                <div className="flex space-x-2">
                  <input
                    type="text"
                    value={transcript}
                    onChange={(e) => setTranscript(e.target.value)}
                    onKeyDown={(e) => {
                      if (e.key === 'Enter' && transcript.trim()) {
                        handleSendMessage();
                      }
                    }}
                    placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."
                    className="flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent"
                  />
                      <Button
                    onClick={() => handleSendMessage()}
                    disabled={!transcript.trim()}
                  >
                    <Send className="h-4 w-4" />
                      </Button>
                  </div>
                  </div>
                </CardContent>
              </Card>
          </div>
        </div>
    </div>
  );
};

export default Voice;
