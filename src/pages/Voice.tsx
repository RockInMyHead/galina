import Header from "@/components/Header";
import { Button } from "@/components/ui/button";
import { Card, CardContent } from "@/components/ui/card";
import { Mic, MicOff, Sparkles, Send } from "lucide-react";
import { useState, useEffect, useRef, useCallback } from "react";
import { sendChatMessage, textToSpeech, playAudioBlob } from "@/utils/apiUtils";
import { AI_SYSTEM_MESSAGES, API_CONFIG } from "@/config/constants";

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
}

const Voice = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  const [showTestMode, setShowTestMode] = useState(false);
  
  // Auto-send timer
  const autoSendTimerRef = useRef<NodeJS.Timeout | null>(null);
  const SILENCE_TIMEOUT = 2000; // 2 seconds
  const [autoSendStatus, setAutoSendStatus] = useState<'idle' | 'waiting' | 'sending'>('idle');

  // Development helpers
  const isLocalhost = location.hostname === 'localhost' || location.hostname === '127.0.0.1';
  const isSecure = window.isSecureContext || location.protocol === 'https:';

  // Detailed logging of environment detection
  useEffect(() => {
    console.log('üîç Environment detection:', {
      hostname: location.hostname,
      protocol: location.protocol,
      port: location.port,
      href: location.href,
      isLocalhost,
      isSecure,
      secureContext: window.isSecureContext,
      userAgent: navigator.userAgent.substring(0, 50) + '...'
    });

    if (isLocalhost && !isSecure) {
      console.log('üîß Auto-enabling test mode for localhost development');
      setShowTestMode(true);
    } else {
      console.log('‚ÑπÔ∏è Environment check:', {
        isLocalhost,
        isSecure,
        reason: !isLocalhost ? 'not localhost' : 'already secure or HTTPS'
      });
    }
  }, [isLocalhost, isSecure]);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      content: '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –ì–∞–ª–∏–Ω–∞, –≤–∞—à AI-—é—Ä–∏—Å—Ç. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –ª—é–±–æ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –≥–æ–ª–æ—Å–æ–º, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å –≤–∞–º —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–µ–π.',
      role: 'assistant',
      timestamp: new Date()
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [isGeneratingTTS, setIsGeneratingTTS] = useState(false);
  const [isPlayingTTS, setIsPlayingTTS] = useState(false);
  const beepIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Audio feedback functions
  const playBeep = useCallback(() => {
    if (!('AudioContext' in window) && !('webkitAudioContext' in window)) return;

    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);

      oscillator.frequency.value = 800;
      oscillator.type = 'sine';

      gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.1);

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.1);
    } catch (error) {
      console.warn('Could not play beep:', error);
    }
  }, []);

  const startBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current);
        }
    beepIntervalRef.current = setInterval(playBeep, 3000); // Every 3 seconds
  }, [playBeep]);

  const stopBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current);
      beepIntervalRef.current = null;
    }
  }, []);

  // Initialize Web Speech API
  useEffect(() => {
    console.log('üîß Initializing Web Speech API...');
    console.log('üìä Browser capabilities:', {
      speechRecognition: !!window.SpeechRecognition,
      webkitSpeechRecognition: !!(window as any).webkitSpeechRecognition,
      mediaDevices: !!navigator.mediaDevices,
      getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)
    });

    const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error('‚ùå Web Speech API not supported in this browser');
      return;
    }

    console.log('‚úÖ Web Speech API supported, creating recognition instance...');

    try {
      const recognition = new SpeechRecognition();
      console.log('üéØ Recognition instance created:', {
        continuous: recognition.continuous,
        interimResults: recognition.interimResults,
        lang: recognition.lang,
        maxAlternatives: recognition.maxAlternatives,
        serviceURI: recognition.serviceURI,
        grammars: recognition.grammars
      });

      recognition.continuous = false; // Use single-shot mode for better reliability
      recognition.interimResults = true;
      recognition.lang = 'ru-RU';
      recognition.maxAlternatives = 1;

      console.log('‚öôÔ∏è Recognition configured:', {
        continuous: recognition.continuous,
        interimResults: recognition.interimResults,
        lang: recognition.lang,
        maxAlternatives: recognition.maxAlternatives
      });

      recognition.onstart = () => {
        console.log('üé§ Speech recognition started successfully');
        setIsRecording(true);
        // isContinuousListening is already set by toggleVoiceMode
      };

      recognition.onresult = (event) => {
        console.log('üìù Speech recognition result received');

        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (interimTranscript) {
          setInterimTranscript(interimTranscript);
        }

        if (finalTranscript && finalTranscript.trim()) {
          console.log('‚úÖ Final transcript:', finalTranscript.trim());

          // Update transcript and immediately schedule auto-send
          setTranscript(prev => {
            const newTranscript = prev ? `${prev} ${finalTranscript.trim()}` : finalTranscript.trim();
          setInterimTranscript('');

            // Clear existing auto-send timer
            if (autoSendTimerRef.current) {
              clearTimeout(autoSendTimerRef.current);
              console.log('üïê Cleared previous auto-send timer');
            }

            // Set status to waiting
            setAutoSendStatus('waiting');

            // Start new auto-send timer for 2 seconds of silence
            autoSendTimerRef.current = setTimeout(() => {
              if (newTranscript.trim()) {
                console.log('‚è∞ Auto-sending after 2 seconds of silence:', newTranscript.trim());
                setAutoSendStatus('sending');
                handleSendMessage(newTranscript.trim());
              }
            }, SILENCE_TIMEOUT);
            console.log('‚è±Ô∏è Started auto-send timer (2 seconds)');

            return newTranscript;
          });

          // If continuous listening is enabled, restart recognition after a delay
          if (isContinuousListening) {
            console.log('üîÑ Continuous mode: restarting recognition in 1 second...');
            setTimeout(() => {
              if (isContinuousListening && recognitionRef.current) {
                try {
                  recognitionRef.current.start();
                } catch (error) {
                  console.error('‚ùå Failed to restart recognition:', error);
                  setIsContinuousListening(false);
                }
              }
            }, 1000); // 1 second delay to prevent conflicts
          }
        }
      };

      recognition.onerror = async (event) => {
        console.error('‚ùå Speech recognition error:', event.error, event);
        console.error('‚ùå Error type:', event.type);
        console.error('‚ùå Error message:', event.message || 'No message');

        // Detailed debug information
        const debugInfo = {
          isLocalhost,
          isSecure,
          hostname: location.hostname,
          protocol: location.protocol,
          port: location.port,
          href: location.href,
          secureContext: window.isSecureContext,
          webkitSpeechRecognition: typeof (window as any).webkitSpeechRecognition,
          speechRecognition: typeof window.SpeechRecognition,
          userAgent: navigator.userAgent,
          timestamp: new Date().toISOString(),
          errorDetails: {
            error: event.error,
            message: event.message,
            type: event.type
          }
        };

        console.log('üîç Full error event object:', JSON.stringify(debugInfo, null, 2));

        // Network connectivity test
        console.log('üåê Testing network connectivity...');
        try {
          const testUrls = [
            'https://www.google.com/favicon.ico',
            'https://www.gstatic.com/speech-api/models/manifest.json',
            'https://clients5.google.com/v1/speech:recognize'
          ];
          
          for (const url of testUrls) {
            try {
              const startTime = performance.now();
              await fetch(url, { method: 'HEAD', mode: 'no-cors' });
              const endTime = performance.now();
              console.log(`‚úÖ Network test passed for ${url} (${Math.round(endTime - startTime)}ms)`);
            } catch (fetchError) {
              console.error(`‚ùå Network test failed for ${url}:`, fetchError);
            }
          }
        } catch (networkError) {
          console.error('‚ùå Network connectivity test error:', networkError);
        }

        // Check browser permissions
        console.log('üîê Checking browser permissions...');
        if (navigator.permissions) {
          try {
            const micPermission = await navigator.permissions.query({ name: 'microphone' as PermissionName });
            console.log('üé§ Microphone permission status:', micPermission.state);
            
            // Try to get more detailed permission info
            if (navigator.mediaDevices && navigator.mediaDevices.enumerateDevices) {
              const devices = await navigator.mediaDevices.enumerateDevices();
              const audioInputs = devices.filter(device => device.kind === 'audioinput');
              console.log('üéôÔ∏è Available audio input devices:', audioInputs.length);
              audioInputs.forEach((device, idx) => {
                console.log(`  ${idx + 1}. ${device.label || `Device ${idx + 1}`} (${device.deviceId.substring(0, 20)}...)`);
              });
            }
          } catch (permError) {
            console.error('‚ùå Permission check error:', permError);
          }
        }

        // Provide more specific error handling
        switch (event.error) {
          case 'network':
            console.error('üîó Network error - check internet connection');
            console.log('üí° Possible causes:');
            console.log('   - No internet connection');
            console.log('   - Firewall blocking Google Speech API');
            console.log('   - VPN interfering with speech services');
            console.log('   - Regional restrictions');
            break;
          case 'not-allowed':
            console.error('üö´ Microphone access denied - check permissions');
            console.log('üí° To fix:');
            console.log('   - Click the üîí icon in address bar');
            console.log('   - Allow microphone access');
            console.log('   - Refresh the page');
            break;
          case 'no-speech':
            console.error('ü§´ No speech detected');
            // This is not critical, just restart if continuous
            if (isContinuousListening) {
              setTimeout(() => startListening(), 1000);
              return;
            }
            break;
          case 'aborted':
            console.error('üõë Recognition was aborted');
            console.log('üîç Detailed debug info:', debugInfo);
            console.table(debugInfo.errorDetails);

            // Detect Safari
            const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent) ||
                            /Safari/i.test(navigator.userAgent) && !/Chrome/i.test(navigator.userAgent);

            if (isSafari) {
              console.log('üß≠ Browser detected: Safari');
              console.log('üí° Safari has known limitations with Web Speech API:');
              console.log('   - May require user interaction before starting recognition');
              console.log('   - May have stricter security policies');
              console.log('   - May block speech models even with network access');
              console.log('   - Often works better in Chrome or Edge');
              console.log('');
              console.log('üîß Safari-specific fixes:');
              console.log('   1. Try Chrome or Edge instead of Safari');
              console.log('   2. Make sure you clicked somewhere on the page first');
              console.log('   3. Try Safari in private browsing mode');
              console.log('   4. Check Safari settings ‚Üí Privacy ‚Üí Microphone');
              console.log('   5. Update Safari to the latest version');
            } else {
              console.log('üí° "Failed to access assets" usually means:');
              console.log('   1. Browser cannot download Google Speech models');
              console.log('   2. Network/Firewall is blocking https://www.gstatic.com');
              console.log('   3. VPN or proxy interfering');
              console.log('   4. Regional restrictions (some countries)');
              console.log('   5. Browser settings blocking third-party requests');
              console.log('');
              console.log('üîß Recommended fixes:');
              console.log('   1. Try disabling VPN/proxy');
              console.log('   2. Check firewall settings');
              console.log('   3. Try Chrome in Incognito mode (no extensions)');
              console.log('   4. Check if you can access https://www.google.com');
              console.log('   5. Try different network (mobile hotspot)');
            }

            console.log('');
            console.log('üíª Enabling test mode for manual text input...');
            console.log('üí° Test mode works in all browsers and doesn\'t require speech recognition!');

            setShowTestMode(true);
            break;
          case 'audio-capture':
            console.error('üéôÔ∏è Audio capture failed - check microphone');
            console.log('üí° Possible causes:');
            console.log('   - Microphone is being used by another app');
            console.log('   - Microphone hardware issue');
            console.log('   - Microphone drivers need update');
            break;
          case 'service-not-allowed':
            console.error('üö´ Speech recognition service not allowed');
            console.log('üí° This might be due to:');
            console.log('   - Browser policy restrictions');
            console.log('   - Corporate/school network blocking');
            break;
          default:
            console.error('‚ùì Unknown error:', event.error);
        }

        setIsRecording(false);
        setIsContinuousListening(false);
      };

      recognition.onend = () => {
        console.log('üõë Speech recognition ended');
        setIsRecording(false);
        // Don't automatically restart here - it's handled in onresult for continuous mode
        // Only disable continuous listening if it was a manual stop
      };

      recognitionRef.current = recognition;

    } catch (error) {
      console.error('‚ùå Failed to initialize speech recognition:', error);
    }
    
    // Cleanup function
    return () => {
      if (autoSendTimerRef.current) {
        clearTimeout(autoSendTimerRef.current);
        console.log('üßπ Cleaned up auto-send timer on unmount');
      }
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  // Manage beep interval based on app state - beep only during AI/TTS processing, not during playback
  useEffect(() => {
    // Start beep when loading (AI thinking) or when TTS is being generated, but NOT during TTS playback
    if ((isLoading || isGeneratingTTS) && !isPlayingTTS) {
      startBeepInterval();
    } else {
      stopBeepInterval();
    }

    // Cleanup on unmount
    return () => {
      stopBeepInterval();
    };
  }, [isLoading, isGeneratingTTS, isPlayingTTS, startBeepInterval, stopBeepInterval]);

  // TTS function for AI responses using OpenAI with parallel generation
  const speakAIResponse = async (text: string) => {
    try {
      console.log('üéµ Preparing parallel OpenAI TTS for AI response...');
      setIsGeneratingTTS(true);

      // Split text into sentences
      const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
      console.log('üìù Split into', sentences.length, 'sentences for parallel TTS');

      // Process and generate TTS for each sentence in parallel
      const ttsPromises = sentences.map(async (sentence, index) => {
        const cleanSentence = sentence.trim();
        if (cleanSentence.length === 0) return null;

        console.log(`üéµ Generating TTS for sentence ${index + 1}/${sentences.length}: "${cleanSentence.substring(0, 50)}..."`);

        // Process text for better speech synthesis
        const processedSentence = processTextForSpeech(cleanSentence);

        try {
          const audioBlob = await textToSpeech(processedSentence);
          return { audio: audioBlob, text: processedSentence, index };
    } catch (error) {
          console.error(`‚ùå Failed to generate TTS for sentence ${index + 1}:`, error);
          return null;
        }
      });

      // Wait for all TTS generations to complete
      console.log('‚è≥ Waiting for all parallel TTS generations...');
      const results = await Promise.allSettled(ttsPromises);

      // Play sentences sequentially
      console.log('‚ñ∂Ô∏è Starting sequential playback...');
      console.log('üé¨ VIDEO SHOULD APPEAR NOW - setIsPlayingTTS(true)');
      setIsPlayingTTS(true);

      for (const result of results) {
        if (result.status === 'fulfilled' && result.value?.audio) {
          const { audio, index } = result.value;
          console.log(`üéµ Playing sentence ${index + 1}, size: ${audio.size} bytes`);
          console.log(`üîä AUDIO SHOULD PLAY NOW for sentence ${index + 1}`);
          await playAudioBlob(audio);
          console.log(`‚úÖ Finished playing sentence ${index + 1}`);

          // Small pause between sentences
          if (index < results.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 200));
          }
        }
      }

      setIsPlayingTTS(false);
      console.log('üé¨ VIDEO SHOULD DISAPPEAR NOW - setIsPlayingTTS(false)');

      console.log('‚úÖ Parallel TTS completed for all sentences');
    } catch (error) {
      console.error('‚ùå Error in parallel OpenAI TTS:', error);
    } finally {
      setIsGeneratingTTS(false);
      setIsPlayingTTS(false);
    }
  };

  // Process text for better speech synthesis
  const processTextForSpeech = (text: string): string => {
    // Convert numbers to words for better pronunciation
    text = text.replace(/\b\d+\b/g, (match) => {
      const num = parseInt(match);
      return numberToWords(num);
    });

    // Convert dates to natural speech
    text = text.replace(/(\d{1,2})\.(\d{1,2})\.(\d{4})/g, (match, day, month, year) => {
      const months = ['—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
                     '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è'];
      return `${parseInt(day)} ${months[parseInt(month) - 1]} ${year} –≥–æ–¥–∞`;
    });

    // Convert mathematical expressions
    text = text.replace(/(\d+)\s*\+\s*(\d+)\s*=\s*(\d+)/g, '$1 –ø–ª—é—Å $2 —Ä–∞–≤–Ω–æ $3');
    text = text.replace(/(\d+)\s*\*\s*(\d+)\s*=\s*(\d+)/g, '$1 —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ $2 —Ä–∞–≤–Ω–æ $3');
    text = text.replace(/(\d+)\s*\/\s*(\d+)\s*=\s*(\d+)/g, '$1 —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ $2 —Ä–∞–≤–Ω–æ $3');

    return text;
  };

  // Convert number to words (simplified Russian)
  const numberToWords = (num: number): string => {
    const units = ['', '–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å'];
    const teens = ['–¥–µ—Å—è—Ç—å', '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å', '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å',
                   '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—å', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—å'];
    const tens = ['', '', '–¥–≤–∞–¥—Ü–∞—Ç—å', '—Ç—Ä–∏–¥—Ü–∞—Ç—å', '—Å–æ—Ä–æ–∫', '–ø—è—Ç—å–¥–µ—Å—è—Ç', '—à–µ—Å—Ç—å–¥–µ—Å—è—Ç', '—Å–µ–º—å–¥–µ—Å—è—Ç', '–≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç', '–¥–µ–≤—è–Ω–æ—Å—Ç–æ'];
    const hundreds = ['', '—Å—Ç–æ', '–¥–≤–µ—Å—Ç–∏', '—Ç—Ä–∏—Å—Ç–∞', '—á–µ—Ç—ã—Ä–µ—Å—Ç–∞', '–ø—è—Ç—å—Å–æ—Ç', '—à–µ—Å—Ç—å—Å–æ—Ç', '—Å–µ–º—å—Å–æ—Ç', '–≤–æ—Å–µ–º—å—Å–æ—Ç', '–¥–µ–≤—è—Ç—å—Å–æ—Ç'];

    if (num === 0) return '–Ω–æ–ª—å';
    if (num < 10) return units[num];
    if (num < 20) return teens[num - 10];
    if (num < 100) return tens[Math.floor(num / 10)] + (num % 10 ? ` ${  units[num % 10]}` : '');
    if (num < 1000) return hundreds[Math.floor(num / 100)] + (num % 100 ? ` ${  numberToWords(num % 100)}` : '');

    return num.toString(); // Fallback for larger numbers
  };

  // Voice control functions
  const startListening = useCallback(async () => {
    console.log('üéØ startListening called, current state:', {
      isRecording,
      recognitionExists: !!recognitionRef.current,
      continuousListening: isContinuousListening
    });

    if (recognitionRef.current && !isRecording) {
      try {
        console.log('‚ñ∂Ô∏è Starting speech recognition process...');

        // Check microphone permissions first
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
          try {
            console.log('üéôÔ∏è Requesting microphone permission...');
            const stream = await navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
              }
            });
            console.log('‚úÖ Microphone permission granted, testing audio context...');

            // Test audio context
            const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
            console.log('üéµ Audio context class available:', !!AudioContextClass);

            const audioContext = new AudioContextClass();
            console.log('üéµ Audio context created:', {
              state: audioContext.state,
              sampleRate: audioContext.sampleRate,
              baseLatency: audioContext.baseLatency
            });

            if (audioContext.state === 'suspended') {
              console.log('üîÑ Audio context suspended, attempting to resume...');
              await audioContext.resume();
              console.log('‚úÖ Audio context resumed, new state:', audioContext.state);
            }

            // Test stream properties
            const audioTracks = stream.getAudioTracks();
            console.log('üéôÔ∏è Audio tracks:', audioTracks.length);
            if (audioTracks.length > 0) {
              const track = audioTracks[0];
              console.log('üéôÔ∏è Audio track settings:', {
                enabled: track.enabled,
                muted: track.muted,
                readyState: track.readyState,
                contentHint: track.contentHint
              });
            }

            stream.getTracks().forEach(track => track.stop());
            await audioContext.close();
            console.log('‚úÖ Audio context test successful');
          } catch (permError) {
            console.error('üö´ Microphone permission denied:', permError);
            console.error('üö´ Error name:', permError.name);
            console.error('üö´ Error message:', permError.message);
            alert('–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.');
            return;
          }
        } else {
          console.error('‚ùå getUserMedia not supported');
          alert('–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
          return;
        }

        // Check if we're in a secure context
        const currentIsLocalhost = location.hostname === 'localhost' || location.hostname === '127.0.0.1';
        const currentIsSecure = window.isSecureContext || location.protocol === 'https:';

        console.log('üîí Security context check:', {
          hostname: location.hostname,
          protocol: location.protocol,
          isSecure: currentIsSecure,
          secureContext: window.isSecureContext,
          isLocalhost: currentIsLocalhost
        });

        if (!currentIsSecure) {
          console.log('‚ö†Ô∏è Not in secure context - Web Speech API may not work');
          console.log(`üìç Current protocol: ${location.protocol}`);
          console.log(`üîí Secure context: ${window.isSecureContext}`);

          if (isLocalhost) {
            console.log('üí° For localhost development, you can:');
            console.log('   1. Use HTTPS: npm run dev -- --https');
            console.log('   2. Configure Chrome: chrome://flags/#unsafely-treat-insecure-origin-as-secure + http://localhost:3001');
            console.log('   3. Use Firefox with media.webspeech.recognition.force_allow_insecure = true');

            // For localhost, we'll try anyway but warn the user
            console.log('üîÑ Trying to use speech recognition despite insecure context...');
          } else {
            alert('Web Speech API —Ç—Ä–µ–±—É–µ—Ç HTTPS —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º.');
            return;
          }
        }

        console.log('‚è≥ Delaying recognition start by 100ms...');
        setTimeout(() => {
          console.log('üöÄ Attempting to start recognition, final check:', {
            recognitionExists: !!recognitionRef.current,
            isRecording,
            continuousListening: isContinuousListening
          });

          if (recognitionRef.current && !isRecording) {
            try {
        recognitionRef.current.start();
              console.log('üé§ Recognition.start() called successfully');
            } catch (startError) {
              console.error('‚ùå Failed to start recognition:', startError);
              console.error('‚ùå Start error details:', {
                name: startError.name,
                message: startError.message,
                stack: startError.stack
              });
            }
          } else {
            console.log('‚ö†Ô∏è Recognition not started - conditions not met');
          }
        }, 100);

      } catch (error) {
        console.error('‚ùå Error starting speech recognition:', error);
      }
    }
  }, [isRecording]);

  const stopListening = useCallback(() => {
      console.log('üõë Stopping speech recognition');
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    setIsRecording(false);
    setIsContinuousListening(false);
    setInterimTranscript('');
  }, []);

  // Handle audio recording completion (legacy MediaRecorder - keeping for compatibility)
  const handleAudioRecorded = async (audioBlob: Blob) => {
    // Since we now use Web Speech API, this function is mainly for legacy compatibility
    // The actual transcription happens in the Web Speech API result handler
    console.log('üé§ Legacy audio processing called, but using Web Speech API instead');
    setIsProcessingAudio(false);
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
  const compressAudio = async (audioBlob: Blob): Promise<Blob> => {
    return new Promise((resolve, reject) => {
      try {
        // Simple passthrough for now - in a real implementation you'd compress
        resolve(audioBlob);
      } catch (error) {
        reject(error);
      }
    });
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
  const transcribeAudioOnServer = async (audioBlob: Blob): Promise<string> => {
    // Since we're using Web Speech API, this is just a placeholder
    return '';
  };

  // Audio recording functions
  const startRecording = async () => {
    try {
      console.log('üé§ Starting audio recording...');

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });

      const selectedMimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: selectedMimeType
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: selectedMimeType });
        console.log('üé§ Recording stopped, processing audio...', {
          size: audioBlob.size,
          type: audioBlob.type
        });
        handleAudioRecorded(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start(1000); // –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–∞–∑–º–µ—Ä–∞
      setIsRecording(true);

    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      alert('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω.');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      console.log('üõë Stopping audio recording...');
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
  const handleSendMessage = async (messageText: string) => {
    if (!messageText.trim()) return;

    console.log('üéØ handleSendMessage called with:', messageText);
    
    // Clear auto-send timer when sending manually or automatically
    if (autoSendTimerRef.current) {
      clearTimeout(autoSendTimerRef.current);
      autoSendTimerRef.current = null;
      console.log('üïê Cleared auto-send timer on send');
    }
    
    // Note: TTS playback interruption will be handled by the audio system
    // when new audio starts playing
    
    setIsLoading(true);

    // Add user message
    const userMessage: Message = {
      id: Date.now().toString(),
      content: messageText,
      role: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setTranscript('');
    setInterimTranscript('');
    setAutoSendStatus('idle');

    try {
      console.log('üöÄ Calling AI API...');
      
      // Prepare conversation history with context
      const conversationHistory = [
        {
              role: 'system',
              content: AI_SYSTEM_MESSAGES.voice
        },
        // Add all previous messages for context
        ...messages.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        // Add current user message
        {
              role: 'user',
              content: messageText
            }
      ];
      
      console.log('üìù Sending conversation with history:', conversationHistory.length, 'messages');
      
      // Generate session ID for conversation memory
      const sessionId = `voice-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;

      // Call AI API with full conversation history
      const apiUrl = `${API_CONFIG.BASE_URL}/chat`;
      console.log('üîó Making API request to:', apiUrl);
      console.log('üìä API_CONFIG.BASE_URL:', API_CONFIG.BASE_URL);

      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'X-Session-ID': sessionId,
        },
        body: JSON.stringify({
          messages: conversationHistory,
          model: 'gpt-5.1',
          reasoning: 'medium',
          temperature: 0.7,
          max_tokens: 2000
        }),
      });

      if (!response.ok) {
        const errorText = await response.text().catch(() => 'Unknown error');
        console.error('‚ùå API error:', response.status, errorText);
        throw new Error(`API error: ${response.status} - ${errorText}`);
      }

      console.log('üì• API response received, parsing...');
      const data = await response.json();
      console.log('üìÑ Raw API response received successfully');
      console.log('üí¨ AI response extracted:', `${data.choices?.[0]?.message?.content?.substring(0, 100)  }...`);

      const aiResponse = data.choices?.[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.';
      console.log('üí¨ AI response extracted:', `${aiResponse.substring(0, 100)  }...`);

      // Add AI response
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: aiResponse,
        role: 'assistant',
        timestamp: new Date()
      };

      console.log('‚úÖ Adding AI message to chat...');
      setMessages(prev => {
        console.log('üìä Total messages after adding AI response:', prev.length + 1);
        return [...prev, assistantMessage];
      });

      // Speak the AI response using OpenAI TTS
      console.log('üîä Starting OpenAI TTS for AI response...');
      console.log('üéµ TTS text length:', aiResponse.length, 'characters');
      console.log('üé¨ About to call speakAIResponse, isPlayingTTS should change to true');
      console.log('‚ñ∂Ô∏è CALLING speakAIResponse NOW...');
      await speakAIResponse(aiResponse);
      console.log('‚úÖ TTS function completed - AUDIO SHOULD BE PLAYING');

    } catch (error) {
      console.error('‚ùå Error in handleSendMessage:', error);

      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.',
        role: 'assistant',
        timestamp: new Date()
      };

      console.log('üö® Adding error message to chat...');
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      console.log('üèÅ handleSendMessage finished');
      setIsLoading(false);
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
  const toggleVoiceMode = async () => {
    console.log('üéõÔ∏è toggleVoiceMode called, isContinuousListening:', isContinuousListening, 'isLoading:', isLoading);

    if (isContinuousListening) {
      console.log('üõë Stopping continuous listening via toggle');
      stopListening();
    } else if (!isLoading) {
      console.log('‚ñ∂Ô∏è Starting continuous listening via toggle');
      setTranscript(''); // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
      setInterimTranscript('');
      startListening();
    } else {
      console.log('‚ùå Cannot start voice recognition: isLoading=', isLoading);
    }
  };

  return (
    <div className="min-h-screen flex flex-col bg-muted/20">
      <Header />
      
      <main className="flex-1">
        <div className="container mx-auto px-4 py-12">
          {/* Header Section */}
          <div className="mb-12 text-center space-y-4">
            <div className="flex justify-center mb-4">
              <div className="h-16 w-16 bg-primary/10 rounded-full flex items-center justify-center">
                <Mic className="h-8 w-8 text-primary" />
              </div>
            </div>
            <h1 className="text-3xl font-bold text-foreground">
              –ì–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ
            </h1>
            <p className="text-muted-foreground max-w-2xl mx-auto">
              –ì–æ–≤–æ—Ä–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –≤–∞—à—É —Ä–µ—á—å –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –≥–æ–ª–æ—Å–æ–º.
            </p>
          </div>

          {/* Main Interface */}
          <div className="max-w-4xl mx-auto">
            <Card className="border-border/50">
                <CardContent className="p-12">
                <div className="text-center space-y-8">
                    {/* Voice Visualizer */}
                    <div className="relative">
                      <video
                        className="h-64 w-64 rounded-full object-cover cursor-pointer mx-auto shadow-2xl"
                        autoPlay
                        loop
                        muted
                        playsInline
                        onClick={toggleVoiceMode}
                      >
                        <source src="/Untitled Video-2.mp4" type="video/mp4" />
                        –í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∏–¥–µ–æ.
                      </video>
                      {/* Overlay for TTS state indication */}
                      {isPlayingTTS && (
                        <div className="absolute inset-0 rounded-full bg-green-500/20 border-4 border-green-500 animate-pulse" />
                      )}
                    </div>

                    <div className="text-center space-y-2">
                      <h2 className="text-2xl font-bold text-foreground">
                      {isLoading ? "" : ""}
                      </h2>
                      <p className="text-muted-foreground">
                      {isLoading ? "" : ""}
                      </p>
                    </div>


                  {/* Test input for development */}
                  {(showTestMode || (isLocalhost && !isSecure)) && (
                    <div className="bg-yellow-50 rounded-lg p-4 border border-yellow-200 mb-4">
                      <div className="text-sm text-yellow-800 mb-2 font-medium">
                        üß™ –†–µ–∂–∏–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ {showTestMode ? '(–≤–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏)' : '(–±–µ–∑ HTTPS)'}
                      </div>
                      <div className="text-xs text-yellow-700 mb-3">
                        {showTestMode
                          ? '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ AI –æ—Ç–≤–µ—Ç–æ–≤.'
                          : 'Speech API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ HTTPS. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.'
                        }
                      </div>
                      <div className="flex gap-2">
                        <input
                          type="text"
                          placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç..."
                          className="flex-1 px-3 py-2 border rounded text-sm"
                          onKeyDown={(e) => {
                            if (e.key === 'Enter' && e.currentTarget.value.trim()) {
                              setTranscript(e.currentTarget.value.trim());
                              e.currentTarget.value = '';
                            }
                          }}
                        />
                        <button
                          onClick={() => {
                            const testTexts = [
                              '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ø–æ–º–æ–≥–∏—Ç–µ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä',
                              '–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä?',
                              '–ö–∞–∫ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –û–û–û?',
                              '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ä–∞–∑–≤–æ–¥–∞?'
                            ];
                            const randomText = testTexts[Math.floor(Math.random() * testTexts.length)];
                            setTranscript(randomText);
                            console.log('üß™ Test input:', randomText);
                          }}
                          className="px-3 py-2 bg-yellow-600 text-white rounded text-sm hover:bg-yellow-700"
                        >
                          üé≤ –°–ª—É—á–∞–π–Ω—ã–π
                        </button>
                      </div>
                    </div>
                  )}

                  {/* Transcript display */}
                  {(transcript || interimTranscript) && (
                    <div className="bg-muted/50 rounded-lg p-4 border">
                      <div className="text-sm text-muted-foreground mb-2">–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:</div>
                      <div className="space-y-2">
                        {transcript && (
                          <textarea
                            value={transcript}
                            onChange={(e) => setTranscript(e.target.value)}
                            className="w-full p-2 rounded border bg-background text-foreground resize-none"
                            rows={3}
                            placeholder="–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç..."
                          />
                        )}
                        {interimTranscript && (
                          <div className="text-muted-foreground italic text-sm">
                            {interimTranscript}
                          </div>
                        )}
                      </div>
                      <div className="flex gap-2 mt-3">
                        {transcript && !isLoading && (
                          <Button
                            size="sm"
                            onClick={() => handleSendMessage(transcript)}
                            disabled={isLoading}
                          >
                            <Send className="h-4 w-4 mr-2" />
                            –û—Ç–ø—Ä–∞–≤–∏—Ç—å
                          </Button>
                        )}
                        {transcript && (
                          <Button
                            size="sm"
                            variant="outline"
                            onClick={() => {
                              setTranscript('');
                              setInterimTranscript('');
                              setAutoSendStatus('idle');
                              if (autoSendTimerRef.current) {
                                clearTimeout(autoSendTimerRef.current);
                                autoSendTimerRef.current = null;
                              }
                            }}
                          >
                            –û—á–∏—Å—Ç–∏—Ç—å
                          </Button>
                        )}
                        {isRecording && (
                          <Button
                            size="sm"
                            variant="outline"
                            onClick={() => {
                              stopListening();
                              setTranscript('');
                              setInterimTranscript('');
                              setAutoSendStatus('idle');
                              if (autoSendTimerRef.current) {
                                clearTimeout(autoSendTimerRef.current);
                                autoSendTimerRef.current = null;
                              }
                            }}
                          >
                            –°–±—Ä–æ—Å–∏—Ç—å
                          </Button>
                        )}
                      </div>

                      {/* Status indicator */}
                      <div className="mt-3 text-xs text-muted-foreground">
                        {isRecording ? (
                          <span className="text-red-500">üî¥ –ó–∞–ø–∏—Å—å –∞–∫—Ç–∏–≤–Ω–∞</span>
                        ) : isContinuousListening ? (
                          <span className="text-yellow-500">üü° –ì–æ—Ç–æ–≤ –∫ –∑–∞–ø–∏—Å–∏</span>
                        ) : autoSendStatus === 'waiting' ? (
                          <span className="text-blue-500">‚è±Ô∏è –ê–≤—Ç–æ–æ—Ç–ø—Ä–∞–≤–∫–∞ —á–µ—Ä–µ–∑ 2 —Å–µ–∫</span>
                        ) : autoSendStatus === 'sending' ? (
                          <span className="text-green-500">üì§ –û—Ç–ø—Ä–∞–≤–∫–∞...</span>
                        ) : (
                          <span className="text-gray-500">‚ö™ –ù–µ–∞–∫—Ç–∏–≤–µ–Ω</span>
                        )}
                      </div>
                    </div>
                  )}

                  {/* Action buttons */}
                  <div className="flex gap-4 justify-center">
                      <Button
                        size="lg"
                        variant={isLoading ? "secondary" : isRecording ? "destructive" : "default"}
                        onClick={toggleVoiceMode}
                        disabled={isLoading}
                        className="shadow-elegant"
                      >
                        {isLoading ? (
                          <Sparkles className="h-5 w-5 mr-2 animate-spin" />
                        ) : isRecording ? (
                          <MicOff className="h-5 w-5 mr-2" />
                        ) : (
                          <Mic className="h-5 w-5 mr-2" />
                        )}
                        {isLoading ? "–û–±—Ä–∞–±–æ—Ç–∫–∞..." :
                       isContinuousListening ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ" :
                       transcript ? "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–∞–ø–∏—Å—å" : "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"}
                      </Button>
                  </div>
                  </div>
                </CardContent>
              </Card>
          </div>
        </div>
      </main>
    </div>
  );
};

export default Voice;