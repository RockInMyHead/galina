import Header from "@/components/Header";
import { Button } from "@/components/ui/button";
import { Card, CardContent } from "@/components/ui/card";
import { Mic, MicOff, Sparkles } from "lucide-react";
import { useState, useEffect, useRef, useCallback } from "react";
import { sendChatMessage } from "@/utils/apiUtils";
import { AI_SYSTEM_MESSAGES } from "@/config/constants";

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
}

const Voice = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      content: '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –ì–∞–ª–∏–Ω–∞, –≤–∞—à AI-—é—Ä–∏—Å—Ç. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –ª—é–±–æ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –≥–æ–ª–æ—Å–æ–º, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å –≤–∞–º —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–µ–π.',
      role: 'assistant',
      timestamp: new Date()
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [reasoningText, setReasoningText] = useState('');

  // Initialize Web Speech API
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;

    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'ru-RU';
      recognition.maxAlternatives = 1;

      recognition.onstart = () => {
        console.log('üé§ Speech recognition started');
        setIsRecording(true);
        setIsContinuousListening(true);
      };

      recognition.onresult = (event) => {
        console.log('üìù Speech recognition result received');

        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (interimTranscript) {
          setInterimTranscript(interimTranscript);
        }

        if (finalTranscript && finalTranscript.trim()) {
          console.log('‚úÖ Final transcript:', finalTranscript.trim());
          setTranscript(finalTranscript.trim());
          setInterimTranscript('');
          // Automatically send the message when speech recognition is complete
          handleSendMessage(finalTranscript.trim());
        }
      };

      recognition.onerror = (event) => {
        console.error('‚ùå Speech recognition error:', event.error);
        setIsRecording(false);
        setIsContinuousListening(false);
      };

      recognition.onend = () => {
        console.log('üõë Speech recognition ended');
        setIsRecording(false);
        setIsContinuousListening(false);
      };

      recognitionRef.current = recognition;
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  // Voice control functions
  const startListening = useCallback(() => {
    if (recognitionRef.current && !isRecording) {
      try {
        console.log('‚ñ∂Ô∏è Starting continuous speech recognition');
        recognitionRef.current.start();
      } catch (error) {
        console.error('‚ùå Error starting speech recognition:', error);
      }
    }
  }, [isRecording]);

  const stopListening = useCallback(() => {
    if (recognitionRef.current && isRecording) {
      console.log('üõë Stopping speech recognition');
      recognitionRef.current.stop();
    }
  }, [isRecording]);

  // Handle audio recording completion (legacy MediaRecorder - keeping for compatibility)
  const handleAudioRecorded = async (audioBlob: Blob) => {
    // Since we now use Web Speech API, this function is mainly for legacy compatibility
    // The actual transcription happens in the Web Speech API result handler
    console.log('üé§ Legacy audio processing called, but using Web Speech API instead');
    setIsProcessingAudio(false);
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
  const compressAudio = async (audioBlob: Blob): Promise<Blob> => {
    return new Promise((resolve, reject) => {
      try {
        // Simple passthrough for now - in a real implementation you'd compress
        resolve(audioBlob);
      } catch (error) {
        reject(error);
      }
    });
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
  const transcribeAudioOnServer = async (audioBlob: Blob): Promise<string> => {
    // Since we're using Web Speech API, this is just a placeholder
    return '';
  };

  // Audio recording functions
  const startRecording = async () => {
    try {
      console.log('üé§ Starting audio recording...');

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });

      const selectedMimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: selectedMimeType
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: selectedMimeType });
        console.log('üé§ Recording stopped, processing audio...', {
          size: audioBlob.size,
          type: audioBlob.type
        });
        handleAudioRecorded(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start(1000); // –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–∞–∑–º–µ—Ä–∞
      setIsRecording(true);

    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      alert('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω.');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      console.log('üõë Stopping audio recording...');
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
  const handleSendMessage = async (messageText: string) => {
    if (!messageText.trim()) return;

    setIsLoading(true);
    setReasoningText('–ì–∞–ª–∏–Ω–∞ –¥—É–º–∞–µ—Ç...');

    // Add user message
    const userMessage: Message = {
      id: Date.now().toString(),
      content: messageText,
      role: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setTranscript('');

    try {
      // Simulate AI reasoning steps
      const reasoningSteps = [
        '–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –≤–∞—à –≤–æ–ø—Ä–æ—Å...',
        '–ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...',
        '–§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞...'
      ];

      for (let i = 0; i < reasoningSteps.length; i++) {
        const step = reasoningSteps[i].trim();
        if (step.length > 0) {
          setReasoningText(step);
          await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 400));
        }
      }

      // Call AI API
      const response = await fetch('http://localhost:3001/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: [
            {
              role: 'system',
              content: AI_SYSTEM_MESSAGES.voice
            },
            {
              role: 'user',
              content: messageText
            }
          ],
          model: 'gpt-3.5-turbo',
          temperature: 0.7,
          max_tokens: 2000
        }),
      });

      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      const aiResponse = data.choices?.[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.';

      // Add AI response
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: aiResponse,
        role: 'assistant',
        timestamp: new Date()
      };

      setMessages(prev => [...prev, assistantMessage]);

    } catch (error) {
      console.error('Error sending message:', error);

      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.',
        role: 'assistant',
        timestamp: new Date()
      };

      setMessages(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
      setReasoningText('');
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
  const toggleVoiceMode = async () => {
    console.log('üéõÔ∏è toggleVoiceMode called, isContinuousListening:', isContinuousListening, 'isLoading:', isLoading);

    if (isContinuousListening) {
      console.log('üõë Stopping continuous listening via toggle');
      stopListening();
    } else if (!isLoading) {
      console.log('‚ñ∂Ô∏è Starting continuous listening via toggle');
      setTranscript(''); // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
      setInterimTranscript('');
      startListening();
    } else {
      console.log('‚ùå Cannot start voice recognition: isLoading=', isLoading);
    }
  };

  return (
    <div className="min-h-screen flex flex-col bg-muted/20">
      <Header />

      <main className="flex-1">
        <div className="container mx-auto px-4 py-12">
          {/* Header Section */}
          <div className="mb-12 text-center space-y-4">
            <div className="flex justify-center mb-4">
              <div className="h-16 w-16 bg-primary/10 rounded-full flex items-center justify-center">
                <Mic className="h-8 w-8 text-primary" />
              </div>
            </div>
            <h1 className="text-3xl font-bold text-foreground">
              –ì–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ
            </h1>
            <p className="text-muted-foreground max-w-2xl mx-auto">
              –ì–æ–≤–æ—Ä–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –≤–∞—à—É —Ä–µ—á—å –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –≥–æ–ª–æ—Å–æ–º.
            </p>
          </div>

          {/* Main Interface */}
          <div className="max-w-4xl mx-auto">
            <Card className="border-border/50">
              <CardContent className="p-12">
                <div className="text-center space-y-8">
                  {/* Voice Visualizer */}
                  <div className="relative">
                    <div
                      onClick={toggleVoiceMode}
                      className={`relative flex h-32 w-32 items-center justify-center rounded-full transition-smooth cursor-pointer mx-auto ${
                        isRecording
                          ? "bg-red-500 shadow-glow animate-pulse"
                          : "bg-primary/10 hover:bg-primary/20"
                      }`}
                    >
                      {isLoading ? (
                        <Sparkles className="h-8 w-8 text-primary animate-spin" />
                      ) : isRecording ? (
                        <MicOff className="h-8 w-8 text-white" />
                      ) : (
                        <Mic className="h-8 w-8 text-primary" />
                      )}
                    </div>
                  </div>

                  <div className="text-center space-y-2">
                    <h2 className="text-2xl font-bold text-foreground">
                      {isLoading ? reasoningText :
                       isRecording ? "üé§ –°–ª—É—à–∞—é..." :
                       "–ì–æ—Ç–æ–≤–∞ —Å–ª—É—à–∞—Ç—å"}
                    </h2>
                    <p className="text-muted-foreground">
                      {isLoading ? reasoningText :
                        isRecording
                        ? "üé§ –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ - –≥–æ–≤–æ—Ä–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ!"
                        : "–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –¥–ª—è –Ω–∞—á–∞–ª–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è"}
                    </p>
                  </div>

                  {isContinuousListening && (
                    <div className="mt-4 p-3 bg-blue-50 border border-blue-200 rounded-lg">
                      <p className="text-sm text-blue-700 font-medium mb-1">
                        üé§ –°–ª—É—à–∞—é...
                      </p>
                      {interimTranscript && (
                        <div className="mt-2 text-sm text-blue-700 italic">
                          "{interimTranscript}"
                        </div>
                      )}
                      <p className="text-blue-600 italic mt-2">–ì–æ–≤–æ—Ä–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, —è —Å–ª—É—à–∞—é</p>
                    </div>
                  )}

                  {isProcessingAudio && (
                    <div className="mt-4 p-3 bg-orange-50 border border-orange-200 rounded-lg">
                      <p className="text-sm text-orange-700 font-medium mb-1">üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ...</p>
                      <p className="text-orange-600 italic">–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —á–µ—Ä–µ–∑ AI</p>
                    </div>
                  )}

                  {/* Action buttons */}
                  <div className="flex gap-4 justify-center">
                    <Button
                      size="lg"
                      variant={isLoading ? "secondary" : isRecording ? "destructive" : "default"}
                      onClick={toggleVoiceMode}
                      disabled={isLoading}
                      className="shadow-elegant"
                    >
                      {isLoading ? (
                        <Sparkles className="h-5 w-5 mr-2 animate-spin" />
                      ) : isRecording ? (
                        <MicOff className="h-5 w-5 mr-2" />
                      ) : (
                        <Mic className="h-5 w-5 mr-2" />
                      )}
                      {isLoading ? "–û–±—Ä–∞–±–æ—Ç–∫–∞..." :
                       isContinuousListening ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ" : "–ù–∞—á–∞—Ç—å –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ"}
                    </Button>
                  </div>
                </div>
              </CardContent>
            </Card>

            {/* Conversation History */}
            <Card className="border-border/50 mt-8">
              <CardContent className="p-6">
                <h3 className="text-lg font-semibold text-foreground mb-4">
                  –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
                </h3>
                <div className="min-h-[300px] max-h-[400px] overflow-y-auto space-y-4">
                  {messages.length === 1 && !isRecording && !transcript ? (
                    <div className="text-center py-12">
                      <Mic className="h-12 w-12 text-muted-foreground mx-auto opacity-50 mb-4" />
                      <p className="text-muted-foreground">
                        –ù–∞—á–Ω–∏—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
                      </p>
                    </div>
                  ) : (
                    <>
                      {messages.map((message) => (
                        <div key={message.id} className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}>
                          <div className={`max-w-[80%] rounded-lg p-4 ${
                            message.role === 'user'
                              ? 'bg-primary text-primary-foreground'
                              : 'bg-muted text-foreground'
                          }`}>
                            <div className="flex items-center gap-2 mb-2">
                              <span className="text-xs font-medium">
                                {message.role === 'user' ? '–í—ã' : '–ì–∞–ª–∏–Ω–∞'}
                              </span>
                              <span className="text-xs opacity-70">
                                {message.timestamp.toLocaleTimeString()}
                              </span>
                            </div>
                            <div className="text-sm leading-relaxed">
                              {message.content}
                            </div>
                          </div>
                        </div>
                      ))}
                    </>
                  )}
                </div>
              </CardContent>
            </Card>
          </div>
        </div>
      </main>
    </div>
  );
};

export default Voice;