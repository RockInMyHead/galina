import Header from "@/components/Header";
import { Button } from "@/components/ui/button";
import { Card, CardContent } from "@/components/ui/card";
import { Mic, MicOff, Sparkles, Send } from "lucide-react";
import { useState, useEffect, useRef, useCallback } from "react";
import { sendChatMessage, textToSpeech, playAudioBlob } from "@/utils/apiUtils";
import { AI_SYSTEM_MESSAGES, API_CONFIG } from "@/config/constants";

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
}

const Voice = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [isContinuousListening, setIsContinuousListening] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  const [showTestMode, setShowTestMode] = useState(false);
  
  // Auto-send timer
  const autoSendTimerRef = useRef<NodeJS.Timeout | null>(null);
  const SILENCE_TIMEOUT = 2000; // 2 seconds
  const [autoSendStatus, setAutoSendStatus] = useState<'idle' | 'waiting' | 'sending'>('idle');

  // Development helpers
  const isLocalhost = typeof window !== 'undefined' && window.location ?
    (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') : false;
  const isSecure = typeof window !== 'undefined' ?
    (window.isSecureContext || (window.location && window.location.protocol === 'https:')) : false;

  // Detailed logging of environment detection
  useEffect(() => {
    if (typeof window !== 'undefined') {
      console.log('üîç Environment detection:', {
        hostname: window.location.hostname,
        protocol: window.location.protocol,
        port: window.location.port,
        href: window.location.href,
        isLocalhost,
        isSecure,
        secureContext: window.isSecureContext,
        userAgent: navigator.userAgent.substring(0, 50) + '...'
      });
    }

    if (isLocalhost && !isSecure) {
      console.log('üîß Auto-enabling test mode for localhost development');
      setShowTestMode(true);
    } else {
      console.log('‚ÑπÔ∏è Environment check:', {
        isLocalhost,
        isSecure,
        reason: !isLocalhost ? 'not localhost' : 'already secure or HTTPS'
      });
    }
  }, [isLocalhost, isSecure]);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      content: '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –ì–∞–ª–∏–Ω–∞, –≤–∞—à AI-—é—Ä–∏—Å—Ç. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –≥–æ–ª–æ—Å–æ–º.',
      role: 'assistant',
      timestamp: new Date()
    }
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [isGeneratingTTS, setIsGeneratingTTS] = useState(false);
  const [isPlayingTTS, setIsPlayingTTS] = useState(false);
  const beepIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Audio feedback functions
  const playBeep = useCallback(() => {
    if (!('AudioContext' in window) && !('webkitAudioContext' in window)) return;

    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);

      oscillator.frequency.value = 800;
      oscillator.type = 'sine';

      gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.1);

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.1);
    } catch (error) {
      console.warn('Could not play beep:', error);
    }
  }, []);

  const startBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current);
        }
    beepIntervalRef.current = setInterval(playBeep, 3000); // Every 3 seconds
  }, [playBeep]);

  const stopBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current);
      beepIntervalRef.current = null;
    }
  }, []);

  // Initialize Web Speech API
  useEffect(() => {
    console.log('üîß Initializing Web Speech API...');
    console.log('üìä Browser capabilities:', {
      speechRecognition: !!window.SpeechRecognition,
      webkitSpeechRecognition: !!(window as any).webkitSpeechRecognition,
      mediaDevices: !!navigator.mediaDevices,
      getUserMedia: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)
    });

    const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error('‚ùå Web Speech API not supported in this browser');
      return;
    }

    console.log('‚úÖ Web Speech API supported, creating recognition instance...');

    try {
      const recognition = new SpeechRecognition();
      console.log('üéØ Recognition instance created:', {
        continuous: recognition.continuous,
        interimResults: recognition.interimResults,
        lang: recognition.lang,
        maxAlternatives: recognition.maxAlternatives,
        serviceURI: recognition.serviceURI,
        grammars: recognition.grammars
      });

      recognition.continuous = false; // Use single-shot mode for better reliability
      recognition.interimResults = true;
      recognition.lang = 'ru-RU';
      recognition.maxAlternatives = 1;

      console.log('‚öôÔ∏è Recognition configured:', {
        continuous: recognition.continuous,
        interimResults: recognition.interimResults,
        lang: recognition.lang,
        maxAlternatives: recognition.maxAlternatives
      });

      recognition.onstart = () => {
        console.log('üé§ Speech recognition started successfully');
        setIsRecording(true);
        // isContinuousListening is already set by toggleVoiceMode
      };

      recognition.onresult = (event) => {
        console.log('üìù Speech recognition result received');

        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        if (interimTranscript) {
          setInterimTranscript(interimTranscript);
        }

        if (finalTranscript && finalTranscript.trim()) {
          console.log('‚úÖ Final transcript:', finalTranscript.trim());
          console.log('üîç Raw transcript data:', {
            finalTranscript,
            trimmed: finalTranscript.trim(),
            length: finalTranscript.length,
            charCodes: [...finalTranscript.trim()].map(c => c.charCodeAt(0))
          });

          // Update transcript and immediately schedule auto-send
          setTranscript(prev => {
            const newTranscript = prev ? `${prev} ${finalTranscript.trim()}` : finalTranscript.trim();
          setInterimTranscript('');

            // Clear existing auto-send timer
            if (autoSendTimerRef.current) {
              clearTimeout(autoSendTimerRef.current);
              console.log('üïê Cleared previous auto-send timer');
            }

            // Set status to waiting
            setAutoSendStatus('waiting');

            // Start new auto-send timer for 2 seconds of silence
            autoSendTimerRef.current = setTimeout(() => {
              if (newTranscript.trim()) {
                console.log('‚è∞ Auto-sending after 2 seconds of silence:', newTranscript.trim());
                setAutoSendStatus('sending');
                handleSendMessage(newTranscript.trim(), true);
              }
            }, SILENCE_TIMEOUT);
            console.log('‚è±Ô∏è Started auto-send timer (2 seconds)');

            return newTranscript;
          });

        // If continuous listening is enabled, restart recognition after a delay
        console.log('üîç Checking continuous listening in onresult:', isContinuousListening);
        if (isContinuousListening) {
          console.log('üîÑ Continuous mode: restarting recognition in 1 second...');
          setTimeout(() => {
            console.log('‚è∞ Timeout triggered, checking conditions...');
            console.log('üîç isContinuousListening:', isContinuousListening, 'recognition exists:', !!recognitionRef.current, 'isRecording:', isRecording);
            if (isContinuousListening && recognitionRef.current && !isRecording) {
              try {
                console.log('‚ñ∂Ô∏è Actually restarting speech recognition...');
                recognitionRef.current.start();
              } catch (error) {
                console.error('‚ùå Failed to restart recognition:', error);
                setIsContinuousListening(false);
              }
            } else {
              console.log('‚ö†Ô∏è Conditions not met for restart');
            }
          }, 1000); // 1 second delay to prevent conflicts
        } else {
          console.log('‚ÑπÔ∏è Continuous listening disabled in onresult');
        }
        }
      };

      recognition.onerror = async (event) => {
        console.error('‚ùå Speech recognition error:', event.error, event);
        console.error('‚ùå Error type:', event.type);
        console.error('‚ùå Error message:', event.message || 'No message');

        // Detailed debug information
        const debugInfo = {
          isLocalhost,
          isSecure,
          hostname: typeof window !== 'undefined' ? window.location.hostname : 'unknown',
          protocol: typeof window !== 'undefined' ? window.location.protocol : 'unknown',
          port: typeof window !== 'undefined' ? window.location.port : 'unknown',
          href: typeof window !== 'undefined' ? window.location.href : 'unknown',
          secureContext: typeof window !== 'undefined' ? window.isSecureContext : false,
          webkitSpeechRecognition: typeof (window as any).webkitSpeechRecognition,
          speechRecognition: typeof window.SpeechRecognition,
          userAgent: navigator.userAgent,
          timestamp: new Date().toISOString(),
          errorDetails: {
            error: event.error,
            message: event.message,
            type: event.type
          }
        };

        console.log('üîç Full error event object:', JSON.stringify(debugInfo, null, 2));

        // Network connectivity test
        console.log('üåê Testing network connectivity...');
        try {
          const testUrls = [
            'https://www.google.com/favicon.ico',
            'https://www.gstatic.com/speech-api/models/manifest.json',
            'https://clients5.google.com/v1/speech:recognize'
          ];
          
          for (const url of testUrls) {
            try {
              const startTime = performance.now();
              await fetch(url, { method: 'HEAD', mode: 'no-cors' });
              const endTime = performance.now();
              console.log(`‚úÖ Network test passed for ${url} (${Math.round(endTime - startTime)}ms)`);
            } catch (fetchError) {
              console.error(`‚ùå Network test failed for ${url}:`, fetchError);
            }
          }
        } catch (networkError) {
          console.error('‚ùå Network connectivity test error:', networkError);
        }

        // Check browser permissions
        console.log('üîê Checking browser permissions...');
        if (navigator.permissions) {
          try {
            const micPermission = await navigator.permissions.query({ name: 'microphone' as PermissionName });
            console.log('üé§ Microphone permission status:', micPermission.state);
            
            // Try to get more detailed permission info
            if (navigator.mediaDevices && navigator.mediaDevices.enumerateDevices) {
              const devices = await navigator.mediaDevices.enumerateDevices();
              const audioInputs = devices.filter(device => device.kind === 'audioinput');
              console.log('üéôÔ∏è Available audio input devices:', audioInputs.length);
              audioInputs.forEach((device, idx) => {
                console.log(`  ${idx + 1}. ${device.label || `Device ${idx + 1}`} (${device.deviceId.substring(0, 20)}...)`);
              });
            }
          } catch (permError) {
            console.error('‚ùå Permission check error:', permError);
          }
        }

        // Detect Safari for error handling
        const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent) ||
                        /Safari/i.test(navigator.userAgent) && !/Chrome/i.test(navigator.userAgent);

        // Provide more specific error handling
        switch (event.error) {
          case 'network':
            console.error('üîó Network error - check internet connection');
            console.log('üí° Possible causes:');
            console.log('   - No internet connection');
            console.log('   - Firewall blocking Google Speech API');
            console.log('   - VPN interfering with speech services');
            console.log('   - Regional restrictions');
            break;
          case 'not-allowed':
            console.error('üö´ Microphone access denied - check permissions');
            console.log('üí° To fix:');
            console.log('   - Click the üîí icon in address bar');
            console.log('   - Allow microphone access');
            console.log('   - Refresh the page');
            break;
          case 'no-speech':
            console.error('ü§´ No speech detected');
            // This is recoverable - will be handled by the centralized error recovery logic
            break;
          case 'aborted':
            console.error('üõë Recognition was aborted');
            console.log('üîç Detailed debug info:', debugInfo);

            // Safari detection already done above

            if (isSafari) {
              console.log('üß≠ Browser detected: Safari');
              console.log('‚ö†Ô∏è Safari often aborts speech recognition due to system conflicts');
              console.log('üí° This is usually NOT a critical error in Safari');
              console.log('üîç DEBUG: isContinuousListening in Safari error handler:', isContinuousListening);

              // For Safari, ALWAYS try to restart recognition since "aborted" is recoverable
              console.log('üîÑ Attempting to restart speech recognition for Safari (forced)...');
              setTimeout(() => {
                console.log('‚è∞ Safari restart timeout triggered');
                console.log('üîç DEBUG: isRecording at restart time:', isRecording);
                if (!isRecording) {
                  console.log('‚ñ∂Ô∏è Restarting speech recognition after Safari abort...');
                  try {
                    startListening();
                    console.log('‚úÖ Successfully restarted speech recognition');
                  } catch (restartError) {
                    console.error('‚ùå Failed to restart after Safari abort:', restartError);
                    // Only then fall back to manual mode
                    console.log('üíª Falling back to manual input mode...');
                    setShowTestMode(true);
                  }
                } else {
                  console.log('‚ö†Ô∏è Cannot restart - still recording');
                }
              }, 2000); // Longer delay for Safari
              return; // Don't set isContinuousListening to false
            } else {
              console.log('üí° "Failed to access assets" usually means:');
              console.log('   1. Browser cannot download Google Speech models');
              console.log('   2. Network/Firewall is blocking https://www.gstatic.com');
              console.log('   3. VPN or proxy interfering');
              console.log('   4. Regional restrictions (some countries)');
              console.log('   5. Browser settings blocking third-party requests');
              console.log('');
              console.log('üîß Recommended fixes:');
              console.log('   1. Try disabling VPN/proxy');
              console.log('   2. Check firewall settings');
              console.log('   3. Try Chrome in Incognito mode (no extensions)');
              console.log('   4. Check if you can access https://www.google.com');
              console.log('   5. Try different network (mobile hotspot)');
            }

            console.log('');
            console.log('üíª Enabling test mode for manual text input...');
            console.log('üí° Test mode works in all browsers and doesn\'t require speech recognition!');

            setShowTestMode(true);
            break;
          case 'audio-capture':
            console.error('üéôÔ∏è Audio capture failed - check microphone');
            console.log('üí° Possible causes:');
            console.log('   - Microphone is being used by another app');
            console.log('   - Microphone hardware issue');
            console.log('   - Microphone drivers need update');
            break;
          case 'service-not-allowed':
            console.error('üö´ Speech recognition service not allowed');
            console.log('üí° This might be due to:');
            console.log('   - Browser policy restrictions');
            console.log('   - Corporate/school network blocking');
            break;
          default:
            console.error('‚ùì Unknown error:', event.error);
        }

        setIsRecording(false);

        // Don't disable continuous listening for recoverable errors
        // 'aborted' in Safari and 'no-speech' are usually recoverable
        console.log('üîç DEBUG: Checking recoverable error logic');
        console.log('üîç DEBUG: event.error =', event.error);
        console.log('üîç DEBUG: isSafari =', isSafari);
        console.log('üîç DEBUG: navigator.userAgent =', navigator.userAgent);

        const isRecoverableError = (event.error === 'aborted' && isSafari) ||
                                  (event.error === 'no-speech');

        console.log('üîç DEBUG: isRecoverableError =', isRecoverableError);

        if (!isRecoverableError) {
          console.log('üö´ DEBUG: Disabling continuous listening for non-recoverable error');
          setIsContinuousListening(false);
        } else {
          console.log(`üîÑ DEBUG: Error "${event.error}" is recoverable, keeping continuous listening enabled`);
          console.log('üîÑ DEBUG: Current continuous listening state:', isContinuousListening);
        }
      };

      recognition.onend = () => {
        console.log('üõë Speech recognition ended');
        console.log('üîÑ Current continuous listening state:', isContinuousListening);
        console.log('üîÑ DEBUG: isRecording before setIsRecording(false):', isRecording);
        setIsRecording(false);
        console.log('üîÑ DEBUG: isRecording after setIsRecording(false):', isRecording);

        // For continuous mode, restart recognition if enabled
        if (isContinuousListening) {
          console.log('üîÑ Continuous mode active, restarting recognition in 500ms...');
          setTimeout(() => {
            if (isContinuousListening && recognitionRef.current && !isRecording) {
              try {
                console.log('‚ñ∂Ô∏è Restarting speech recognition...');
                recognitionRef.current.start();
              } catch (error) {
                console.error('‚ùå Failed to restart recognition in onend:', error);
        setIsContinuousListening(false);
              }
            } else {
              console.log('‚ö†Ô∏è Cannot restart: continuous=', isContinuousListening, 'recording=', isRecording);
            }
          }, 500);
        } else {
          console.log('‚ÑπÔ∏è Continuous listening disabled, recognition ended');
        }
      };

      recognitionRef.current = recognition;

    } catch (error) {
      console.error('‚ùå Failed to initialize speech recognition:', error);
    }
    
    // Cleanup function
    return () => {
      if (autoSendTimerRef.current) {
        clearTimeout(autoSendTimerRef.current);
        console.log('üßπ Cleaned up auto-send timer on unmount');
      }
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  // Manage beep interval based on app state - beep only during AI/TTS processing, not during playback
  useEffect(() => {
    // Start beep when loading (AI thinking) or when TTS is being generated, but NOT during TTS playback
    if ((isLoading || isGeneratingTTS) && !isPlayingTTS) {
      startBeepInterval();
    } else {
      stopBeepInterval();
    }

    // Cleanup on unmount
    return () => {
      stopBeepInterval();
    };
  }, [isLoading, isGeneratingTTS, isPlayingTTS, startBeepInterval, stopBeepInterval]);

  // TTS function for AI responses using OpenAI TTS
  const speakAIResponse = async (text: string) => {
    try {
      console.log('üéµ Preparing OpenAI TTS for AI response...');
      setIsGeneratingTTS(true);

      // Split text into sentences
      const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
      console.log('üìù Split into', sentences.length, 'sentences for OpenAI TTS');

      // Start video animation
      console.log('üé¨ VIDEO SHOULD APPEAR NOW - setIsPlayingTTS(true)');
      setIsPlayingTTS(true);

      // Process and generate TTS for each sentence in parallel
      const ttsPromises = sentences.map(async (sentence, index) => {
        const cleanSentence = sentence.trim();
        if (cleanSentence.length === 0) return null;

        console.log(`üéµ Generating OpenAI TTS for sentence ${index + 1}/${sentences.length}: "${cleanSentence.substring(0, 50)}..."`);

        // Process text for better speech synthesis
        const processedSentence = processTextForSpeech(cleanSentence);

        try {
          const audioBlob = await textToSpeech(processedSentence);
          return { audio: audioBlob, text: processedSentence, index };
    } catch (error) {
          console.error(`‚ùå Failed to generate OpenAI TTS for sentence ${index + 1}:`, error);
          return null;
        }
      });

      // Wait for all TTS generations to complete
      console.log('‚è≥ Waiting for all OpenAI TTS generations...');
      const results = await Promise.allSettled(ttsPromises);

      // Play sentences sequentially
      console.log('‚ñ∂Ô∏è Starting sequential playback...');

      for (const result of results) {
        if (result.status === 'fulfilled' && result.value?.audio) {
          const { audio, index } = result.value;
          console.log(`üéµ Playing sentence ${index + 1}, size: ${audio.size} bytes`);
          console.log(`üîä AUDIO SHOULD PLAY NOW for sentence ${index + 1}`);
          await playAudioBlob(audio);
          console.log(`‚úÖ Finished playing sentence ${index + 1}`);

          // Small pause between sentences
          if (index < results.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 200));
          }
        }
      }

      setIsPlayingTTS(false);
      console.log('üé¨ VIDEO SHOULD DISAPPEAR NOW - setIsPlayingTTS(false)');
      console.log('‚úÖ OpenAI TTS completed for all sentences');

    } catch (error) {
      console.error('‚ùå Error in OpenAI TTS:', error);
    } finally {
      setIsGeneratingTTS(false);
      setIsPlayingTTS(false);
    }
  };

  // Process text for better speech synthesis
  const processTextForSpeech = (text: string): string => {
    // Convert numbers to words for better pronunciation
    text = text.replace(/\b\d+\b/g, (match) => {
      const num = parseInt(match);
      return numberToWords(num);
    });

    // Convert dates to natural speech
    text = text.replace(/(\d{1,2})\.(\d{1,2})\.(\d{4})/g, (match, day, month, year) => {
      const months = ['—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
                     '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è'];
      return `${parseInt(day)} ${months[parseInt(month) - 1]} ${year} –≥–æ–¥–∞`;
    });

    // Convert mathematical expressions
    text = text.replace(/(\d+)\s*\+\s*(\d+)\s*=\s*(\d+)/g, '$1 –ø–ª—é—Å $2 —Ä–∞–≤–Ω–æ $3');
    text = text.replace(/(\d+)\s*\*\s*(\d+)\s*=\s*(\d+)/g, '$1 —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ $2 —Ä–∞–≤–Ω–æ $3');
    text = text.replace(/(\d+)\s*\/\s*(\d+)\s*=\s*(\d+)/g, '$1 —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ $2 —Ä–∞–≤–Ω–æ $3');

    return text;
  };

  // Convert number to words (simplified Russian)
  const numberToWords = (num: number): string => {
    const units = ['', '–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å'];
    const teens = ['–¥–µ—Å—è—Ç—å', '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å', '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å',
                   '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—å', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—å'];
    const tens = ['', '', '–¥–≤–∞–¥—Ü–∞—Ç—å', '—Ç—Ä–∏–¥—Ü–∞—Ç—å', '—Å–æ—Ä–æ–∫', '–ø—è—Ç—å–¥–µ—Å—è—Ç', '—à–µ—Å—Ç—å–¥–µ—Å—è—Ç', '—Å–µ–º—å–¥–µ—Å—è—Ç', '–≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç', '–¥–µ–≤—è–Ω–æ—Å—Ç–æ'];
    const hundreds = ['', '—Å—Ç–æ', '–¥–≤–µ—Å—Ç–∏', '—Ç—Ä–∏—Å—Ç–∞', '—á–µ—Ç—ã—Ä–µ—Å—Ç–∞', '–ø—è—Ç—å—Å–æ—Ç', '—à–µ—Å—Ç—å—Å–æ—Ç', '—Å–µ–º—å—Å–æ—Ç', '–≤–æ—Å–µ–º—å—Å–æ—Ç', '–¥–µ–≤—è—Ç—å—Å–æ—Ç'];

    if (num === 0) return '–Ω–æ–ª—å';
    if (num < 10) return units[num];
    if (num < 20) return teens[num - 10];
    if (num < 100) return tens[Math.floor(num / 10)] + (num % 10 ? ` ${  units[num % 10]}` : '');
    if (num < 1000) return hundreds[Math.floor(num / 100)] + (num % 100 ? ` ${  numberToWords(num % 100)}` : '');

    return num.toString(); // Fallback for larger numbers
  };

  // Voice control functions
  const startListening = useCallback(async () => {
    console.log('üéØ startListening called, current state:', {
      isRecording,
      recognitionExists: !!recognitionRef.current,
      continuousListening: isContinuousListening
    });

    if (recognitionRef.current && !isRecording) {
      try {
        console.log('‚ñ∂Ô∏è Starting speech recognition process...');

        // Check microphone permissions first
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
          try {
            console.log('üéôÔ∏è Requesting microphone permission...');
            const stream = await navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
              }
            });
            console.log('‚úÖ Microphone permission granted, testing audio context...');

            // Test audio context
            const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
            console.log('üéµ Audio context class available:', !!AudioContextClass);

            const audioContext = new AudioContextClass();
            console.log('üéµ Audio context created:', {
              state: audioContext.state,
              sampleRate: audioContext.sampleRate,
              baseLatency: audioContext.baseLatency
            });

            if (audioContext.state === 'suspended') {
              console.log('üîÑ Audio context suspended, attempting to resume...');
              await audioContext.resume();
              console.log('‚úÖ Audio context resumed, new state:', audioContext.state);
            }

            // Test stream properties
            const audioTracks = stream.getAudioTracks();
            console.log('üéôÔ∏è Audio tracks:', audioTracks.length);
            if (audioTracks.length > 0) {
              const track = audioTracks[0];
              console.log('üéôÔ∏è Audio track settings:', {
                enabled: track.enabled,
                muted: track.muted,
                readyState: track.readyState,
                contentHint: track.contentHint
              });
            }

            stream.getTracks().forEach(track => track.stop());
            await audioContext.close();
            console.log('‚úÖ Audio context test successful');
          } catch (permError) {
            console.error('üö´ Microphone permission denied:', permError);
            console.error('üö´ Error name:', permError.name);
            console.error('üö´ Error message:', permError.message);
            alert('–î–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.');
            return;
          }
        } else {
          console.error('‚ùå getUserMedia not supported');
          alert('–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É');
          return;
        }

        // Check if we're in a secure context
        const currentIsLocalhost = typeof window !== 'undefined' &&
          (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1');
        const currentIsSecure = typeof window !== 'undefined' &&
          (window.isSecureContext || window.location.protocol === 'https:');

        if (typeof window !== 'undefined') {
          console.log('üîí Security context check:', {
            hostname: window.location.hostname,
            protocol: window.location.protocol,
            isSecure: currentIsSecure,
            secureContext: window.isSecureContext,
            isLocalhost: currentIsLocalhost
          });

          if (!currentIsSecure) {
            console.log('‚ö†Ô∏è Not in secure context - Web Speech API may not work');
            console.log(`üìç Current protocol: ${window.location.protocol}`);
            console.log(`üîí Secure context: ${window.isSecureContext}`);

            if (currentIsLocalhost) {
              console.log('üí° For localhost development, you can:');
              console.log('   1. Use HTTPS: npm run dev -- --https');
              console.log('   2. Configure Chrome: chrome://flags/#unsafely-treat-insecure-origin-as-secure + http://localhost:3001');
              console.log('   3. Use Firefox with media.webspeech.recognition.force_allow_insecure = true');

              // For localhost, we'll try anyway but warn the user
              console.log('üîÑ Trying to use speech recognition despite insecure context...');
            } else {
              alert('Web Speech API —Ç—Ä–µ–±—É–µ—Ç HTTPS —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º.');
              return;
            }
          }
        }

        console.log('‚è≥ Delaying recognition start by 100ms...');
        setTimeout(() => {
          console.log('üöÄ Attempting to start recognition, final check:', {
            recognitionExists: !!recognitionRef.current,
            isRecording,
            continuousListening: isContinuousListening
          });

          if (recognitionRef.current && !isRecording) {
            try {
        recognitionRef.current.start();
              console.log('üé§ Recognition.start() called successfully');
            } catch (startError) {
              console.error('‚ùå Failed to start recognition:', startError);
              console.error('‚ùå Start error details:', {
                name: startError.name,
                message: startError.message,
                stack: startError.stack
              });
            }
          } else {
            console.log('‚ö†Ô∏è Recognition not started - conditions not met');
          }
        }, 100);

      } catch (error) {
        console.error('‚ùå Error starting speech recognition:', error);
      }
    }
  }, [isRecording]);

  const stopListening = useCallback((forcedStop = false) => {
      console.log('üõë Stopping speech recognition', forcedStop ? '(FORCED STOP)' : '(TEMPORARY STOP)');
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    setIsRecording(false);
    // Only disable continuous listening if it's a forced stop (user toggle)
    if (forcedStop) {
      console.log('üö´ Disabling continuous listening (forced stop)');
      setIsContinuousListening(false);
    } else {
      console.log('‚è∏Ô∏è Pausing for processing, continuous listening remains enabled');
    }
    setInterimTranscript('');
  }, []);

  // Handle audio recording completion (legacy MediaRecorder - keeping for compatibility)
  const handleAudioRecorded = async (audioBlob: Blob) => {
    // Since we now use Web Speech API, this function is mainly for legacy compatibility
    // The actual transcription happens in the Web Speech API result handler
    console.log('üé§ Legacy audio processing called, but using Web Speech API instead');
    setIsProcessingAudio(false);
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
  const compressAudio = async (audioBlob: Blob): Promise<Blob> => {
    return new Promise((resolve, reject) => {
      try {
        // Simple passthrough for now - in a real implementation you'd compress
        resolve(audioBlob);
      } catch (error) {
        reject(error);
      }
    });
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
  const transcribeAudioOnServer = async (audioBlob: Blob): Promise<string> => {
    // Since we're using Web Speech API, this is just a placeholder
    return '';
  };

  // Audio recording functions
  const startRecording = async () => {
    try {
      console.log('üé§ Starting audio recording...');

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      });

      const selectedMimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: selectedMimeType
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: selectedMimeType });
        console.log('üé§ Recording stopped, processing audio...', {
          size: audioBlob.size,
          type: audioBlob.type
        });
        handleAudioRecorded(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start(1000); // –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–∞–∑–º–µ—Ä–∞
      setIsRecording(true);

    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      alert('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω.');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      console.log('üõë Stopping audio recording...');
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
  const handleSendMessage = async (messageText: string, isVoice: boolean = false) => {
    if (!messageText.trim()) return;

    console.log('üéØ handleSendMessage called with:', messageText, 'isVoice:', isVoice);
    console.log('üîç Continuous listening state at send:', isContinuousListening);
    console.log('üì§ Message details:', {
      original: messageText,
      trimmed: messageText.trim(),
      length: messageText.length,
      words: messageText.trim().split(' ').length
    });

    // Clear auto-send timer when sending manually or automatically
    if (autoSendTimerRef.current) {
      clearTimeout(autoSendTimerRef.current);
      autoSendTimerRef.current = null;
      console.log('üïê Cleared auto-send timer on send');
    }
    
    // Note: TTS playback interruption will be handled by the audio system
    // when new audio starts playing
    
    setIsLoading(true);

    // Save the continuous listening state at the start
    // If continuous listening is not enabled yet, enable it automatically for voice conversations
    const shouldResumeContinuous = isContinuousListening || isVoice; // Enable if isVoice flag is true OR if continuous was enabled
    console.log('üíæ Saved continuous listening state for resume:', shouldResumeContinuous, '(original:', isContinuousListening, ', isVoice:', isVoice, ')');

    // If this is a voice message and continuous listening wasn't enabled, enable it now
    if (!isContinuousListening && isVoice) {
      console.log('üéØ Auto-enabling continuous listening for voice conversation');
      setIsContinuousListening(true);
    }

    // Add user message
    const userMessage: Message = {
      id: Date.now().toString(),
      content: messageText,
      role: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setTranscript('');
    setInterimTranscript('');
    setAutoSendStatus('idle');

    try {
      console.log('üöÄ Calling AI API...');
      
      // Prepare conversation history with context
      const conversationHistory = [
        {
              role: 'system',
              content: AI_SYSTEM_MESSAGES.voice
        },
        // Add all previous messages for context
        ...messages.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        // Add current user message
        {
              role: 'user',
              content: messageText
            }
      ];
      
      console.log('üìù Sending conversation with history:', conversationHistory.length, 'messages');
      
      // Generate session ID for conversation memory
      const sessionId = `voice-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;

      // Call AI API with full conversation history
      const apiUrl = `${API_CONFIG.BASE_URL}/chat`;
      console.log('üîó Making API request to:', apiUrl);
      console.log('üìä API_CONFIG.BASE_URL:', API_CONFIG.BASE_URL);
      console.log('üîÑ Session ID:', sessionId);
      console.log('üì® Request payload:', JSON.stringify({
        messages: conversationHistory,
        model: 'gpt-5.1',
        temperature: 0.7,
        max_completion_tokens: 2000
      }, null, 2));

      let response;
      try {
        console.log('üöÄ Executing fetch request...');
        console.log('üîó Full URL:', apiUrl);
        console.log('üì¶ Request body:', JSON.stringify({
          messages: conversationHistory,
          model: 'gpt-5.1',
          temperature: 0.7,
          max_completion_tokens: 2000
        }, null, 2));

        // Add timeout to fetch
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 30000); // 30 second timeout

        response = await fetch(apiUrl, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'X-Session-ID': sessionId,
          },
          body: JSON.stringify({
            messages: conversationHistory,
            model: 'gpt-5.1',
            temperature: 0.7,
            max_completion_tokens: 2000
          }),
          signal: controller.signal
        });

        clearTimeout(timeoutId);

        console.log('‚úÖ Fetch completed, response received');
        console.log('üìä Response status:', response.status);
        console.log('üìä Response statusText:', response.statusText);
        console.log('üìä Response headers:', Object.fromEntries(response.headers.entries()));
      } catch (fetchError) {
        console.error('‚ùå Fetch failed with error:', fetchError);
        console.error('‚ùå Error name:', fetchError.name);
        console.error('‚ùå Error message:', fetchError.message);
        if (fetchError.name === 'AbortError') {
          console.error('‚ùå Request timed out after 30 seconds');
        }
        throw new Error(`Network error: ${fetchError.message}`);
      }

      if (!response.ok) {
        console.log('‚ö†Ô∏è Response not OK, reading error body...');
        const errorText = await response.text().catch(() => 'Unknown error');
        console.error('‚ùå API error body:', errorText);
        console.error('‚ùå Full response details:', {
          status: response.status,
          statusText: response.statusText,
          headers: Object.fromEntries(response.headers.entries()),
          body: errorText
        });
        throw new Error(`API error: ${response.status} - ${errorText}`);
      }

      console.log('üì• Response OK, parsing JSON...');
      let data;
      try {
        data = await response.json();
        console.log('üìÑ JSON parsed successfully');
        console.log('üìä Response data keys:', Object.keys(data));
        console.log('üìä Response data:', JSON.stringify(data, null, 2));
      } catch (jsonError) {
        console.error('‚ùå JSON parsing failed:', jsonError);
        throw new Error(`JSON parsing error: ${jsonError.message}`);
      }

      console.log('üí¨ Extracting AI response...');
      const aiResponse = data.choices?.[0]?.message?.content || '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.';
      console.log('üí¨ AI response extracted successfully:', aiResponse.substring(0, 100) + '...');

      // Add AI response
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: aiResponse,
        role: 'assistant',
        timestamp: new Date()
      };

      console.log('‚úÖ Adding AI message to chat...');
      setMessages(prev => {
        console.log('üìä Total messages after adding AI response:', prev.length + 1);
        return [...prev, assistantMessage];
      });

      // Speak the AI response using OpenAI TTS
      console.log('üîä Starting OpenAI TTS for AI response...');
      console.log('üéµ TTS text length:', aiResponse.length, 'characters');
      console.log('üé¨ About to call speakAIResponse, isPlayingTTS should change to true');
      console.log('‚ñ∂Ô∏è CALLING speakAIResponse NOW...');
      await speakAIResponse(aiResponse);
      console.log('‚úÖ TTS function completed - AUDIO SHOULD BE PLAYING');

      // Resume continuous listening after AI response is complete
      console.log('üîÑ CHECKING CONTINUOUS LISTENING RESUMPTION...');
      console.log('üìä CURRENT STATE IMMEDIATELY AFTER TTS:', {
        continuous: isContinuousListening,
        recording: isRecording,
        playingTTS: isPlayingTTS,
        recognitionExists: !!recognitionRef.current
      });

      // Force resume continuous listening using saved state
      // We know TTS has completed because we're in this code block
      if (shouldResumeContinuous) {
        console.log('üîÑ FORCED: Resuming continuous listening after AI response...');
        console.log('üíæ Using saved state (shouldResumeContinuous):', shouldResumeContinuous);
        console.log('üéØ This should enable continuous listening for ongoing voice conversation');

        // Use a longer delay to ensure any audio cleanup is complete
        setTimeout(() => {
          console.log('‚è∞ FORCED RESUMPTION: Timeout triggered');
          console.log('üìä STATE AT FORCED RESUMPTION:', {
            savedContinuous: shouldResumeContinuous,
            currentContinuous: isContinuousListening,
            recording: isRecording,
            playingTTS: isPlayingTTS,
            recognitionExists: !!recognitionRef.current
          });

          // Only check if we're not currently recording
          if (!isRecording) {
            console.log('‚ñ∂Ô∏è FORCED: Starting new listening session after AI response');
            try {
              startListening();
              console.log('‚úÖ FORCED: startListening() called successfully');
            } catch (error) {
              console.error('‚ùå FORCED: Failed to start listening:', error);
            }
          } else {
            console.log('‚ö†Ô∏è FORCED: Cannot resume - currently recording');

            // Try again in another second if still recording
            setTimeout(() => {
              if (shouldResumeContinuous && !isRecording) {
                console.log('‚ñ∂Ô∏è RETRY: Starting listening session after second delay');
                try {
                  startListening();
                } catch (error) {
                  console.error('‚ùå RETRY: Failed to start listening:', error);
                }
              }
            }, 1000);
          }
        }, 2000); // Longer delay to ensure everything is cleaned up
      } else {
        console.log('üö´ CONTINUOUS LISTENING WAS NOT ENABLED - NO RESUMPTION NEEDED');
        console.log('üí° User can manually enable continuous listening via toggle');
      }

    } catch (error) {
      console.error('‚ùå CRITICAL ERROR in handleSendMessage:', error);
      console.error('‚ùå Error type:', error.constructor.name);
      console.error('‚ùå Error message:', error.message);
      console.error('‚ùå Error stack:', error.stack);

      // Additional error details
      if (error instanceof TypeError) {
        console.error('‚ùå This is a TypeError - likely network or parsing issue');
      } else if (error instanceof SyntaxError) {
        console.error('‚ùå This is a SyntaxError - likely JSON parsing issue');
      } else if (error.name === 'AbortError') {
        console.error('‚ùå This is an AbortError - request was aborted');
      } else if (error.name === 'TimeoutError') {
        console.error('‚ùå This is a TimeoutError - request timed out');
      }

      // Check network connectivity
      console.log('üåê Checking network connectivity...');
      fetch('http://127.0.0.1:3003/test-proxy', { method: 'GET' })
        .then(networkResponse => {
          if (networkResponse.ok) {
            console.log('‚úÖ Network connectivity OK - backend reachable');
          } else {
            console.log('‚ö†Ô∏è Network connectivity issue - backend responded with error');
          }
        })
        .catch(networkError => {
          console.error('‚ùå Network connectivity FAILED - cannot reach backend:', networkError);
        });

      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: `–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –î–µ—Ç–∞–ª–∏: ${error.message}`,
        role: 'assistant',
        timestamp: new Date()
      };

      console.log('üö® Adding error message to chat...');
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      console.log('üèÅ handleSendMessage finished');
      setIsLoading(false);
    }
  };

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
  const toggleVoiceMode = async () => {
    console.log('üéõÔ∏è toggleVoiceMode called, isContinuousListening:', isContinuousListening, 'isLoading:', isLoading, 'isRecording:', isRecording, 'playingTTS:', isPlayingTTS);

    // Prevent toggling while TTS is playing to avoid accidental interruption
    if (isPlayingTTS) {
      console.log('üö´ Cannot toggle voice mode while TTS is playing');
      return;
    }

    if (isContinuousListening) {
      console.log('üõë Stopping continuous listening via toggle');
      setIsContinuousListening(false);
      stopListening(true); // Force stop when user toggles
    } else if (!isLoading) {
      console.log('‚ñ∂Ô∏è Starting continuous listening via toggle');
      setTranscript(''); // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
      setInterimTranscript('');
      setIsContinuousListening(true); // –í–∫–ª—é—á–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –Ω–∞–∂–∞—Ç–∏–∏
      console.log('‚úÖ Set isContinuousListening to true');
      startListening();
    } else {
      console.log('‚ùå Cannot start voice recognition: isLoading=', isLoading);
    }
  };

  return (
    <div className="min-h-screen flex flex-col bg-muted/20">
      <Header />
      
      <main className="flex-1">
        <div className="container mx-auto px-4 py-12">
          {/* Header Section */}
          <div className="mb-12 text-center space-y-4">
            <div className="flex justify-center mb-4">
              <div className="h-16 w-16 bg-primary/10 rounded-full flex items-center justify-center">
                <Mic className="h-8 w-8 text-primary" />
              </div>
            </div>
            <h1 className="text-3xl font-bold text-foreground">
              –ì–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ
            </h1>
            <p className="text-muted-foreground max-w-2xl mx-auto">
              –ì–æ–≤–æ—Ä–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ - —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –≤–∞—à—É —Ä–µ—á—å –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –≥–æ–ª–æ—Å–æ–º.
            </p>
          </div>

          {/* Main Interface */}
          <div className="max-w-4xl mx-auto">
            <Card className="border-border/50">
                <CardContent className="p-12">
                <div className="text-center space-y-8">
                    {/* Voice Visualizer */}
                    <div className="relative">
                      <video
                        className="h-64 w-64 rounded-full object-cover cursor-pointer mx-auto shadow-2xl"
                        autoPlay
                        loop
                        muted
                        playsInline
                        onClick={toggleVoiceMode}
                      >
                        <source src="/Untitled Video-2.mp4" type="video/mp4" />
                        –í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∏–¥–µ–æ.
                      </video>
                    </div>

                    <div className="text-center space-y-2">
                      <h2 className="text-2xl font-bold text-foreground">
                      {isLoading ? "" : ""}
                      </h2>
                      <p className="text-muted-foreground">
                      {isLoading ? "" : ""}
                      </p>
                    </div>


                  {/* Test input for development */}
                  {(showTestMode || (isLocalhost && !isSecure)) && (
                    <div className="bg-yellow-50 rounded-lg p-4 border border-yellow-200 mb-4">
                      <div className="text-sm text-yellow-800 mb-2 font-medium">
                        üß™ –†–µ–∂–∏–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ {showTestMode ? '(–≤–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏)' : '(–±–µ–∑ HTTPS)'}
                      </div>
                      <div className="text-xs text-yellow-700 mb-3">
                        {showTestMode
                          ? '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ AI –æ—Ç–≤–µ—Ç–æ–≤.'
                          : 'Speech API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ HTTPS. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.'
                        }
                      </div>
                      <div className="flex gap-2">
                        <input
                          type="text"
                          placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç..."
                          className="flex-1 px-3 py-2 border rounded text-sm"
                          onKeyDown={(e) => {
                            if (e.key === 'Enter' && e.currentTarget.value.trim()) {
                              setTranscript(e.currentTarget.value.trim());
                              e.currentTarget.value = '';
                            }
                          }}
                        />
                        <button
                          onClick={() => {
                            const testTexts = [
                              '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –ø–æ–º–æ–≥–∏—Ç–µ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä',
                              '–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä?',
                              '–ö–∞–∫ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –û–û–û?',
                              '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ä–∞–∑–≤–æ–¥–∞?'
                            ];
                            const randomText = testTexts[Math.floor(Math.random() * testTexts.length)];
                            setTranscript(randomText);
                            console.log('üß™ Test input:', randomText);
                          }}
                          className="px-3 py-2 bg-yellow-600 text-white rounded text-sm hover:bg-yellow-700"
                        >
                          üé≤ –°–ª—É—á–∞–π–Ω—ã–π
                        </button>
                      </div>
                    </div>
                  )}


                  {/* Action buttons */}
                  <div className="flex gap-4 justify-center">
                      <Button
                        size="lg"
                        variant={isLoading ? "secondary" : isRecording ? "destructive" : "default"}
                        onClick={toggleVoiceMode}
                        disabled={isLoading}
                        className="shadow-elegant"
                      >
                        {isLoading ? (
                          <Sparkles className="h-5 w-5 mr-2 animate-spin" />
                        ) : isRecording ? (
                          <MicOff className="h-5 w-5 mr-2" />
                        ) : (
                          <Mic className="h-5 w-5 mr-2" />
                        )}
                        {isLoading ? "–û–±—Ä–∞–±–æ—Ç–∫–∞..." :
                       isContinuousListening ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ" :
                       transcript ? "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∑–∞–ø–∏—Å—å" : "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"}
                      </Button>
                  </div>

                  </div>
                </CardContent>
              </Card>
          </div>
        </div>
      </main>
    </div>
  );
};

export default Voice;