const express = require('express');
const cors = require('cors');

// LangChain imports (Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸)
let ChatOpenAI, AgentExecutor, createOpenAIToolsAgent, TavilySearchResults, ChatPromptTemplate, MessagesPlaceholder;

try {
  ChatOpenAI = require('@langchain/openai').ChatOpenAI;
  ({ AgentExecutor, createOpenAIToolsAgent } = require('langchain/agents'));
  ({ TavilySearchResults } = require('@langchain/community/tools/tavily_search'));
  ({ ChatPromptTemplate, MessagesPlaceholder } = require('@langchain/core/prompts'));
  console.log('âœ… LangChain modules loaded successfully');
} catch (error) {
  console.warn('âš ï¸ LangChain modules not available, using fallback mode:', error.message);
  ChatOpenAI = null;
}

const app = express();
app.use(cors());
app.use(express.json({ limit: '50mb' }));

// LangChain Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸)
let agent = null;

if (ChatOpenAI) {
  class LangChainAgent {
    constructor() {
      this.agentExecutor = null;
      this.llm = new ChatOpenAI({
        modelName: process.env.OPENAI_MODEL || "gpt-4-turbo",
        temperature: 0.3,
        openAIApiKey: process.env.OPENAI_API_KEY,
      });
    }

    async initializeAgent() {
      if (this.agentExecutor) return;

      console.log('ğŸ¤– Initializing LangChain agent...');

      try {
        // Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
        const tools = [
          new TavilySearchResults({
            maxResults: 3,
            apiKey: process.env.TAVILY_API_KEY,
          }),
        ];

        // Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
        const prompt = ChatPromptTemplate.fromMessages([
          ["system", `Ğ’Ñ‹ - Ğ“Ğ°Ğ»Ğ¸Ğ½Ğ°, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-ÑÑ€Ğ¸ÑÑ‚ Ñ 25-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ² Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¹ ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ğ¸.

ĞĞ¡ĞĞ‘Ğ•ĞĞĞĞ¡Ğ¢Ğ˜ Ğ ĞĞ‘ĞĞ¢Ğ«:
- Ğ”Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ´Ğ»Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
- ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹Ñ‚Ğµ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ
- Ğ‘ÑƒĞ´ÑŒÑ‚Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹

Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢Ğ«:
- tavily_search_results_json: Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ

ĞšĞĞ“Ğ”Ğ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ¬ ĞŸĞĞ˜Ğ¡Ğš:
- Ğ”Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ…, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ² Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ
- Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑƒĞ´ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ¾Ğ²
- Ğ”Ğ»Ñ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»ÑƒĞ³Ğ°Ñ…
- ĞšĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹ ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ

Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞĞ¢Ğ’Ğ•Ğ¢Ğ:
- ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ğ¹Ñ‚Ğµ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ
- ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
- Ğ”Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
- ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼ ÑÑ€Ğ¸ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²`],
          ["human", "{input}"],
          new MessagesPlaceholder("agent_scratchpad"),
        ]);

        // Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°
        const agent = await createOpenAIToolsAgent({
          llm: this.llm,
          tools,
          prompt,
        });

        // Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ
        this.agentExecutor = new AgentExecutor({
          agent,
          tools,
          verbose: true,
          maxIterations: 3,
          returnIntermediateSteps: true,
        });

        console.log('âœ… LangChain agent initialized successfully');
      } catch (error) {
        console.error('âŒ Failed to initialize LangChain agent:', error);
        this.agentExecutor = null;
      }
    }

    async processQuery(query, context = []) {
      try {
        await this.initializeAgent();

        if (!this.agentExecutor) {
          throw new Error('Agent not available');
        }

        console.log('ğŸ§  Processing query with LangChain:', query.substring(0, 100) + '...');

        // Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ input Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
        const input = context && context.length > 0
          ? `${query}\n\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°:\n${context.join('\n')}`
          : query;

        // Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
        const result = await this.agentExecutor.invoke({
          input,
        });

        console.log('âœ… LLM response generated with LangChain');

        return {
          success: true,
          content: result.output,
          searchUsed: result.intermediateSteps?.some(step => step.action.tool === 'tavily_search_results_json') || false,
        };

      } catch (error) {
        console.error('âŒ LangChain agent error:', error);
        throw error; // ĞŸÑ€Ğ¾Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ´Ğ»Ñ fallback
      }
    }
  }

  agent = new LangChainAgent();
} else {
  console.log('âš ï¸ LangChain modules not available, using mock responses only');
}

// Fallback Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
function getFallbackResponse(query) {
  const lowerQuery = query.toLowerCase();

  if (lowerQuery.includes('Ğ¾Ğ¾Ğ¾') && lowerQuery.includes('Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†')) {
    return 'Ğ”Ğ»Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞĞĞ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: 1. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. 2. Ğ£ÑÑ‚Ğ°Ğ² ĞĞĞ. 3. Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ğ¾Ğ± ÑƒÑ‡Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ ĞĞĞ (ĞµÑĞ»Ğ¸ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾). 4. Ğ—Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ 11001. 5. ĞšĞ²Ğ¸Ñ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğµ Ğ³Ğ¾ÑĞ¿Ğ¾ÑˆĞ»Ğ¸Ğ½Ñ‹ (4000 Ñ€ÑƒĞ±Ğ»ĞµĞ¹). 6. Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ°Ğ´Ñ€ĞµÑ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. 7. ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ. Ğ’ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ñ Ğ² ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ğ» Ğ“Ğ¾ÑÑƒÑĞ»ÑƒĞ³ Ğ¸Ğ»Ğ¸ ĞœĞ¤Ğ¦.';
  }

  if (lowerQuery.includes('Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚') || lowerQuery.includes('Ğ·Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹')) {
    return 'Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ! Ğ¯ Ğ“Ğ°Ğ»Ğ¸Ğ½Ğ°, ÑĞ»Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ AI-ÑÑ€Ğ¸ÑÑ‚ Ñ 25-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸. Ğ¯ - Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ² Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ. Ğ§ĞµĞ¼ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ? Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¾ Ğ²Ğ°ÑˆĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ñ.';
  }

  return 'Ğ¯ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»ÑƒÑˆĞ°Ñ Ğ²Ğ°ÑˆÑƒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ²Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½ÑƒĞ»Ğ¸ÑÑŒ. ĞšĞ°Ğº Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ€Ğ¸ÑÑ‚, Ñ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ğ°ÑˆÑƒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.';
}

app.post('/chat', async (req, res) => {
  try {
    console.log('=== Chat Request Received ===');
    console.log('Session ID:', req.headers['x-session-id']);
    console.log('Messages count:', req.body?.messages?.length || 0);

    // Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    const messages = req.body?.messages || [];
    const lastUserMessage = messages.filter(m => m.role === 'user').pop()?.content || '';

    // Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° (Ğ²ÑĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ)
    const conversationContext = messages
      .filter(m => m.role === 'assistant')
      .map(m => m.content)
      .slice(-5); // Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 5 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

    console.log('Last user message:', lastUserMessage.substring(0, 100) + '...');
    console.log('Conversation context length:', conversationContext.length);

    let responseContent = '';

    // ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LangChain Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
    if (agent) {
      try {
        const result = await agent.processQuery(lastUserMessage, conversationContext);
        console.log('LLM processing result:', result.success ? 'SUCCESS' : 'FALLBACK');
        console.log('Search used:', result.searchUsed || false);
        responseContent = result.content;
      } catch (error) {
        console.warn('LangChain agent failed, using fallback:', error.message);
        responseContent = getFallbackResponse(lastUserMessage);
      }
    } else {
      // Fallback Ğ´Ğ»Ñ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ğ³Ğ´Ğ° LangChain Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
      console.log('Using fallback responses (LangChain not available)');
      responseContent = getFallbackResponse(lastUserMessage);
    }

    const response = {
      id: agent ? `llm-${Date.now()}` : `mock-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: agent ? 'gpt-4-turbo' : 'gpt-5.1',
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: responseContent,
          refusal: null
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: 100,
        completion_tokens: Math.floor(responseContent.length / 4),
        total_tokens: 100 + Math.floor(responseContent.length / 4)
      }
    };

    console.log('âœ… Sending LLM-powered response');
    res.status(200).json(response);
  } catch (error) {
    console.error('âŒ Chat error:', error);
    res.status(500).json({
      error: 'Internal server error',
      details: error.message,
      fallback: 'ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.'
    });
  }
});

// TTS endpoint
app.post('/tts', (req, res) => {
  try {
    console.log('=== TTS Request Received ===');
    console.log('Text to speak:', req.body?.text?.substring(0, 50) + '...');

    // Mock TTS response - return a small audio blob
    // In a real implementation, this would generate actual TTS audio
    const mockAudioBuffer = Buffer.alloc(1024, 0); // 1KB of zeros as mock audio

    res.set({
      'Content-Type': 'audio/mpeg',
      'Content-Length': mockAudioBuffer.length,
      'Cache-Control': 'no-cache'
    });

    console.log('âœ… Sending mock TTS audio response');
    res.status(200).send(mockAudioBuffer);
  } catch (error) {
    console.error('âŒ TTS error:', error.message);
    res.status(500).json({ error: 'TTS Internal server error', details: error.message });
  }
});

// Health check
app.get('/test-proxy', (req, res) => {
  res.json({ message: 'Proxy is working correctly!' });
});

const PORT = process.env.PORT || 3003;

// Graceful error handling
process.on('uncaughtException', (err) => {
  console.error('âŒ Uncaught Exception:', err);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('âŒ Unhandled Rejection at:', promise, 'reason:', reason);
});

app.listen(PORT, () => {
  console.log(`ğŸš€ Stable API server running on port ${PORT}`);
}).on('error', (err) => {
  console.error('âŒ Server error:', err);
});
