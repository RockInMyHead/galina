require('dotenv').config();
const express = require('express');
const cors = require('cors');

// ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ OpenAI API Ğ¸ Tavily API (Ğ±ĞµĞ· LangChain agents Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ²ĞµÑ€ÑĞ¸Ğ¹)
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const TAVILY_API_KEY = process.env.TAVILY_API_KEY;
const USE_LLM = OPENAI_API_KEY && TAVILY_API_KEY;

if (USE_LLM) {
  console.log('âœ… LLM Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ (OpenAI + Tavily)');
} else {
  console.log('âš ï¸ Fallback Ñ€ĞµĞ¶Ğ¸Ğ¼ (API ĞºĞ»ÑÑ‡Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹)');
}

const app = express();
app.use(cors());
app.use(express.json({ limit: '50mb' }));

// Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Tavily API
async function searchWithTavily(query) {
  try {
    console.log('ğŸ” Searching with Tavily:', query);

    const response = await fetch('https://api.tavily.com/search', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${TAVILY_API_KEY}`,
      },
      body: JSON.stringify({
        query,
        max_results: 3,
        include_answer: true,
      }),
    });

    if (!response.ok) {
      throw new Error(`Tavily API error: ${response.status}`);
    }

    const data = await response.json();

    if (data.results && data.results.length > 0) {
      const formattedResults = data.results.map((result, index) =>
        `[${index + 1}] ${result.title}\nURL: ${result.url}\nĞ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ: ${result.content}\n`
      ).join('\n');

      console.log('ğŸ“Š Found', data.results.length, 'search results');
      return {
        success: true,
        results: formattedResults,
        query: query
      };
    } else {
      return {
        success: false,
        results: "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ.",
        query: query
      };
    }
  } catch (error) {
    console.error('âŒ Tavily search error:', error);
    return {
      success: false,
      results: "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.",
      query: query
    };
  }
}

// Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ‡ĞµÑ€ĞµĞ· OpenAI (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ)
async function processWithLLM(query, context = []) {
  try {
    console.log('ğŸ§  Processing with LLM:', query.substring(0, 100) + '...');

    // Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ OpenAI
    const systemPrompt = `Ğ’Ñ‹ - Ğ“Ğ°Ğ»Ğ¸Ğ½Ğ°, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-ÑÑ€Ğ¸ÑÑ‚ Ñ 25-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ² Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¹ ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ğ¸.

ĞĞ¡ĞĞ‘Ğ•ĞĞĞĞ¡Ğ¢Ğ˜ Ğ ĞĞ‘ĞĞ¢Ğ«:
- Ğ”Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹
- ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹Ñ‚Ğµ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ
- Ğ‘ÑƒĞ´ÑŒÑ‚Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹

Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞĞ¢Ğ’Ğ•Ğ¢Ğ:
- ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ğ¹Ñ‚Ğµ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ
- Ğ”Ğ°Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
- ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¶Ğ¸Ğ²Ñ‹Ğ¼ ÑÑ€Ğ¸ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²

Ğ•ÑĞ»Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ĞºĞ°ÑĞ°ĞµÑ‚ÑÑ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ Ğ³Ğ¾Ğ´ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½.`;

    // Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°
    const contextString = context && context.length > 0
      ? `\n\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°:\n${context.join('\n')}`
      : '';

    const userPrompt = `${query}${contextString}`;

    console.log('ğŸ“¡ Calling OpenAI API...');

    // Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ OpenAI API Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ¼
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000); // 25 ÑĞµĞº Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚

    try {
      const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
        },
        body: JSON.stringify({
          model: 'gpt-4-turbo',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          temperature: 0.3,
          max_tokens: 1000, // Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ°ĞµĞ¼ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸
        }),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      console.log('ğŸ“¡ OpenAI response status:', openaiResponse.status);

      if (!openaiResponse.ok) {
        const errorText = await openaiResponse.text();
        throw new Error(`OpenAI API error: ${openaiResponse.status} - ${errorText}`);
      }

      const openaiData = await openaiResponse.json();
      const content = openaiData.choices[0].message.content;

      console.log('âœ… LLM response generated successfully');

      return {
        success: true,
        content: content,
        searchUsed: false,
        model: 'gpt-4-turbo'
      };

    } catch (fetchError) {
      clearTimeout(timeoutId);
      if (fetchError.name === 'AbortError') {
        throw new Error('OpenAI API timeout');
      }
      throw fetchError;
    }

  } catch (error) {
    console.error('âŒ LLM processing error:', error);
    throw error;
  }
}

// Fallback Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²
function getFallbackResponse(query) {
  const lowerQuery = query.toLowerCase();

  if (lowerQuery.includes('Ğ¾Ğ¾Ğ¾') && lowerQuery.includes('Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†')) {
    return 'Ğ”Ğ»Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞĞĞ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: 1. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. 2. Ğ£ÑÑ‚Ğ°Ğ² ĞĞĞ. 3. Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ğ¾Ğ± ÑƒÑ‡Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ ĞĞĞ (ĞµÑĞ»Ğ¸ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾). 4. Ğ—Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ 11001. 5. ĞšĞ²Ğ¸Ñ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğµ Ğ³Ğ¾ÑĞ¿Ğ¾ÑˆĞ»Ğ¸Ğ½Ñ‹ (4000 Ñ€ÑƒĞ±Ğ»ĞµĞ¹). 6. Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ°Ğ´Ñ€ĞµÑ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. 7. ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ. Ğ’ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ñ Ğ² ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ğ» Ğ“Ğ¾ÑÑƒÑĞ»ÑƒĞ³ Ğ¸Ğ»Ğ¸ ĞœĞ¤Ğ¦.';
  }

  if (lowerQuery.includes('Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚') || lowerQuery.includes('Ğ·Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹')) {
    return 'Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ! Ğ¯ Ğ“Ğ°Ğ»Ğ¸Ğ½Ğ°, ÑĞ»Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ AI-ÑÑ€Ğ¸ÑÑ‚ Ñ 25-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸. Ğ¯ - Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ² Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ. Ğ§ĞµĞ¼ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ğ°Ğ¼ ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ? Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¾ Ğ²Ğ°ÑˆĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ñ.';
  }

  return 'Ğ¯ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»ÑƒÑˆĞ°Ñ Ğ²Ğ°ÑˆÑƒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ²Ñ‹ ÑÑ‚Ğ¾Ğ»ĞºĞ½ÑƒĞ»Ğ¸ÑÑŒ. ĞšĞ°Ğº Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ€Ğ¸ÑÑ‚, Ñ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ğ°ÑˆÑƒ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.';
}

app.post('/chat', async (req, res) => {
  try {
    console.log('=== Chat Request Received ===');
    console.log('Session ID:', req.headers['x-session-id']);
    console.log('Messages count:', req.body?.messages?.length || 0);

    // Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    const messages = req.body?.messages || [];
    const lastUserMessage = messages.filter(m => m.role === 'user').pop()?.content || '';

    // Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° (Ğ²ÑĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ)
    const conversationContext = messages
      .filter(m => m.role === 'assistant')
      .map(m => m.content)
      .slice(-5); // Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 5 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

    console.log('Last user message:', lastUserMessage.substring(0, 100) + '...');
    console.log('Conversation context length:', conversationContext.length);

    let responseContent = '';

    // Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ LLM Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ API ĞºĞ»ÑÑ‡Ğ¸
    let useLLM = false;

    try {
      if (USE_LLM) {
        console.log('ğŸš€ Using LLM...');
        const result = await processWithLLM(lastUserMessage, conversationContext);
        console.log('âœ… LLM processing result: SUCCESS');
        console.log('ğŸ” Search used:', result.searchUsed || false);
        responseContent = result.content;
        useLLM = true;
        console.log('ğŸ¯ LLM response generated successfully');
      } else {
        console.log('âš ï¸ Using fallback responses (API keys not available)');
        responseContent = getFallbackResponse(lastUserMessage);
      }
    } catch (error) {
      console.warn('âŒ LLM processing failed, using fallback:', error.message);
      responseContent = getFallbackResponse(lastUserMessage);
    }

    const response = {
      id: useLLM ? `llm-${Date.now()}` : `mock-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: useLLM ? 'gpt-4-turbo' : 'gpt-5.1',
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: responseContent,
          refusal: null
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: useLLM ? 150 : 10,
        completion_tokens: Math.floor(responseContent.length / 4),
        total_tokens: (useLLM ? 150 : 10) + Math.floor(responseContent.length / 4)
      }
    };

    console.log('âœ… Sending LLM-powered response');
    res.status(200).json(response);
  } catch (error) {
    console.error('âŒ Chat error:', error);
    res.status(500).json({
      error: 'Internal server error',
      details: error.message,
      fallback: 'ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.'
    });
  }
});

// TTS endpoint
app.post('/tts', (req, res) => {
  try {
    console.log('=== TTS Request Received ===');
    console.log('Text to speak:', req.body?.text?.substring(0, 50) + '...');

    // Mock TTS response - return a small audio blob
    // In a real implementation, this would generate actual TTS audio
    const mockAudioBuffer = Buffer.alloc(1024, 0); // 1KB of zeros as mock audio

    res.set({
      'Content-Type': 'audio/mpeg',
      'Content-Length': mockAudioBuffer.length,
      'Cache-Control': 'no-cache'
    });

    console.log('âœ… Sending mock TTS audio response');
    res.status(200).send(mockAudioBuffer);
  } catch (error) {
    console.error('âŒ TTS error:', error.message);
    res.status(500).json({ error: 'TTS Internal server error', details: error.message });
  }
});

// Health check
app.get('/test-proxy', (req, res) => {
  res.json({ message: 'Proxy is working correctly!' });
});

const PORT = process.env.PORT || 3003;

// Graceful error handling
process.on('uncaughtException', (err) => {
  console.error('âŒ Uncaught Exception:', err);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('âŒ Unhandled Rejection at:', promise, 'reason:', reason);
});

app.listen(PORT, () => {
  console.log(`ğŸš€ Stable API server running on port ${PORT}`);
}).on('error', (err) => {
  console.error('âŒ Server error:', err);
});
