// Chat service - handles chat logic and conversation management
const openaiAdapter = require('./openai/adapter');
const { generateMockResponse } = require('./mock');
const { RESPONSE_FORMATS } = require('./openai/config');

// In-memory conversation storage for GPT-5.1 (since it doesn't support conversation history)
const conversationMemory = new Map();

// Helper function to get conversation context
function getConversationContext(sessionId, maxMessages = 10) {
  const conversation = conversationMemory.get(sessionId) || [];
  return conversation.slice(-maxMessages); // Keep only last N messages
}

// Helper function to add message to conversation
function addToConversation(sessionId, message) {
  if (!conversationMemory.has(sessionId)) {
    conversationMemory.set(sessionId, []);
  }
  const conversation = conversationMemory.get(sessionId);
  conversation.push(message);

  // Keep only last 50 messages to prevent memory leaks
  if (conversation.length > 50) {
    conversation.splice(0, conversation.length - 50);
  }
}

class ChatService {
  async processChatRequest({ messages, model, max_completion_tokens, temperature, stream }) {
    console.log('=== New Chat Request ===');
    console.log('Request body:', JSON.stringify({ messages, model, max_completion_tokens, temperature, stream }, null, 2));

    // Check for placeholder API key
    if (!openaiAdapter.apiKey || openaiAdapter.apiKey.trim() === '' || openaiAdapter.apiKey === 'sk-your-actual-openai-api-key-here') {
      console.log('âš ï¸ No valid API key configured for chat - using demo response');
      console.log('ðŸ’¡ To enable real AI responses, set OPENAI_API_KEY in api/.env');

      const lastMessage = messages[messages.length - 1];
      const userContent = lastMessage?.content || '';
      const lowerContent = userContent.toLowerCase();

      let mockContent = '';

      if (lowerContent.includes('Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†') && lowerContent.includes('Ð¾Ð¾Ð¾') ||
          lowerContent.includes('Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚') && lowerContent.includes('Ð¾Ð¾Ð¾') ||
          lowerContent.includes('Ð½ÑƒÐ¶Ð½') && lowerContent.includes('Ð¾Ð¾Ð¾')) {
        mockContent = 'Ð”Ð»Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÐžÐžÐž Ð² Ð Ð¾ÑÑÐ¸Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: Ð£ÑÑ‚Ð°Ð² Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ð°, Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ ÐžÐžÐž, Ð—Ð°ÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ðµ Ð 11001, Ð”Ð¾Ð³Ð¾Ð²Ð¾Ñ€ Ð¾Ð± ÑƒÑ‡Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¸ ÐžÐžÐž (ÐµÑÐ»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑƒÑ‡Ñ€ÐµÐ´Ð¸Ñ‚ÐµÐ»ÐµÐ¹), ÐšÐ²Ð¸Ñ‚Ð°Ð½Ñ†Ð¸Ñ Ð¾Ð± Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð³Ð¾ÑÐ¿Ð¾ÑˆÐ»Ð¸Ð½Ñ‹ (4000 Ñ€ÑƒÐ±Ð»ÐµÐ¹), ÐŸÐ°ÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð¸ Ð˜ÐÐ ÑƒÑ‡Ñ€ÐµÐ´Ð¸Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½Ð° ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð´Ñ€ÐµÑ.';
      } else if (lowerContent.includes('Ð¸Ð¿') || lowerContent.includes('Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½') && lowerContent.includes('Ð¿Ñ€ÐµÐ´Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»')) {
        mockContent = 'Ð”Ð»Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð˜ÐŸ Ð² Ð Ð¾ÑÑÐ¸Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ: Ð—Ð°ÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ðµ Ð 21001, ÐŸÐ°ÑÐ¿Ð¾Ñ€Ñ‚, Ð˜ÐÐ Ð¸ ÐšÐ²Ð¸Ñ‚Ð°Ð½Ñ†Ð¸Ñ Ð¾Ð± Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð³Ð¾ÑÐ¿Ð¾ÑˆÐ»Ð¸Ð½Ñ‹ (800 Ñ€ÑƒÐ±Ð»ÐµÐ¹).';
      } else if (lowerContent.includes('Ð¿Ð»Ð°Ð½') && lowerContent.includes('Ð¾Ñ‚Ð²ÐµÑ‚')) {
        mockContent = '1. ÐŸÑ€Ð°Ð²Ð¾Ð²Ñ‹Ðµ Ð¾ÑÐ½Ð¾Ð²Ñ‹ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹\n2. ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸\n3. Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ Ñ€Ð¸ÑÐºÐ¸ Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ';
      } else if (userContent.trim() === '') {
        mockContent = 'Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿ÑƒÑÑ‚Ð¾Ð¹. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.';
      } else {
        mockContent = `Ð¯ Ð“Ð°Ð»Ð¸Ð½Ð°, Ð²Ð°Ñˆ AI-ÑŽÑ€Ð¸ÑÑ‚. Ð’Ñ‹ ÑÐ¿Ñ€Ð¾ÑÐ¸Ð»Ð¸: "${userContent.substring(0, 100)}${userContent.length > 100 ? '...' : ''}".

Ð”Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð¼Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹ Ð¾ Ð²Ð°ÑˆÐµÐ¹ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚Ðµ:
- ÐšÐ°ÐºÐ¾Ð¹ Ñ‚Ð¸Ð¿ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð²Ð°Ñ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÐµÑ‚?
- Ð’ ÐºÐ°ÐºÐ¾Ð¹ ÑÑ„ÐµÑ€Ðµ Ð¿Ñ€Ð°Ð²Ð° (Ð³Ñ€Ð°Ð¶Ð´Ð°Ð½ÑÐºÐ¾Ðµ, ÑƒÐ³Ð¾Ð»Ð¾Ð²Ð½Ð¾Ðµ, Ñ‚Ñ€ÑƒÐ´Ð¾Ð²Ð¾Ðµ Ð¸ Ñ‚.Ð´.)?
- ÐšÐ°ÐºÐ¸Ðµ Ð¾Ð±ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð¿Ñ€Ð¸Ð²ÐµÐ»Ð¸ Ðº Ð´Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑƒ?

Ð¯ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ð²Ð°Ð¼ Ñ Ð»ÑŽÐ±Ñ‹Ð¼Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¤ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ð¸.`;
      }

      return {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion',
        created: Math.floor(Date.now() / 1000),
        model: 'demo-mode',
        choices: [{
          index: 0,
          message: {
            role: 'assistant',
            content: mockContent
          },
          finish_reason: 'stop'
        }],
        usage: {
          prompt_tokens: messages.reduce((sum, msg) => sum + (msg.content?.length || 0), 0),
          completion_tokens: mockContent.length,
          total_tokens: messages.reduce((sum, msg) => sum + (msg.content?.length || 0), 0) + mockContent.length
        }
      };
    }

    // Use real OpenAI API
    if (!stream) {
      console.log('Processing non-streaming request...');

      const lastMessage = messages[messages.length - 1];
      const isVisionRequest = Array.isArray(lastMessage?.content) &&
                             lastMessage.content.some(item => item.type === 'image_url');

      // Use demo mode only if API key is not valid for non-Vision requests
      const shouldUseDemo = !await openaiAdapter.validateApiKey() && !isVisionRequest;

      if (shouldUseDemo) {
        console.log('âš ï¸ API key not valid and not Vision API request, using demo mode');
        return generateMockResponse(messages, model);
      }

      // For Vision API requests, try real API even if validation failed
      if (isVisionRequest && !await openaiAdapter.validateApiKey()) {
        console.log('ðŸ–¼ï¸ Vision API request with potentially invalid key, trying anyway...');
      }

      try {
        const params = {
          messages,
          model,
          max_completion_tokens,
          temperature
        };

        const result = isVisionRequest
          ? await openaiAdapter.createVisionCompletion(params, RESPONSE_FORMATS.LEGACY)
          : await openaiAdapter.createChatCompletion(params, RESPONSE_FORMATS.LEGACY);

        console.log('OpenAI response received, sending to client');
        return result;
      } catch (error) {
        console.error('OpenAI API error:', error);
        // Fallback to demo mode on API errors
        return generateMockResponse(messages, model);
      }
    } else {
      // Handle streaming requests
      const apiKeyValid = await openaiAdapter.validateApiKey();

      if (!apiKeyValid) {
        // Mock streaming for testing when no API key
        console.log('No API key - using mock streaming for testing');
        return this.createMockStream(messages, model);
      } else {
        // Real streaming with OpenAI
        console.log('Starting real streaming with OpenAI');
        return openaiAdapter.createChatCompletion({
          messages,
          model,
          max_completion_tokens,
          temperature,
          stream: true
        }, RESPONSE_FORMATS.LEGACY);
      }
    }
  }

  async createMockStream(messages, model) {
    // Get smart mock content based on the last message
    const lastMessage = messages[messages.length - 1];
    const userContent = lastMessage?.content || '';
    const lowerContent = userContent.toLowerCase();

    let mockContent = '';

    if (lowerContent.includes('Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†') && lowerContent.includes('Ð¾Ð¾Ð¾') ||
        lowerContent.includes('Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚') && lowerContent.includes('Ð¾Ð¾Ð¾') ||
        lowerContent.includes('Ð½ÑƒÐ¶Ð½') && lowerContent.includes('Ð¾Ð¾Ð¾')) {
      mockContent = 'Ð”Ð»Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÐžÐžÐž Ð² Ð Ð¾ÑÑÐ¸Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: Ð£ÑÑ‚Ð°Ð² Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ð°, Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ ÐžÐžÐž, Ð—Ð°ÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ðµ Ð 11001, Ð”Ð¾Ð³Ð¾Ð²Ð¾Ñ€ Ð¾Ð± ÑƒÑ‡Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¸ ÐžÐžÐž (ÐµÑÐ»Ð¸ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑƒÑ‡Ñ€ÐµÐ´Ð¸Ñ‚ÐµÐ»ÐµÐ¹), ÐšÐ²Ð¸Ñ‚Ð°Ð½Ñ†Ð¸Ñ Ð¾Ð± Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð³Ð¾ÑÐ¿Ð¾ÑˆÐ»Ð¸Ð½Ñ‹ (4000 Ñ€ÑƒÐ±Ð»ÐµÐ¹), ÐŸÐ°ÑÐ¿Ð¾Ñ€Ñ‚Ð° Ð¸ Ð˜ÐÐ ÑƒÑ‡Ñ€ÐµÐ´Ð¸Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð°, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð½Ð° ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð´Ñ€ÐµÑ.';
    } else if (lowerContent.includes('Ð¸Ð¿') || lowerContent.includes('Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½') && lowerContent.includes('Ð¿Ñ€ÐµÐ´Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»')) {
      mockContent = 'Ð”Ð»Ñ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð˜ÐŸ Ð² Ð Ð¾ÑÑÐ¸Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ: Ð—Ð°ÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ„Ð¾Ñ€Ð¼Ðµ Ð 21001, ÐŸÐ°ÑÐ¿Ð¾Ñ€Ñ‚, Ð˜ÐÐ Ð¸ ÐšÐ²Ð¸Ñ‚Ð°Ð½Ñ†Ð¸Ñ Ð¾Ð± Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð³Ð¾ÑÐ¿Ð¾ÑˆÐ»Ð¸Ð½Ñ‹ (800 Ñ€ÑƒÐ±Ð»ÐµÐ¹).';
    } else if (lowerContent.includes('Ð¿Ð»Ð°Ð½') && lowerContent.includes('Ð¾Ñ‚Ð²ÐµÑ‚')) {
      mockContent = '1. ÐŸÑ€Ð°Ð²Ð¾Ð²Ñ‹Ðµ Ð¾ÑÐ½Ð¾Ð²Ñ‹ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹\n2. ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸\n3. Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ Ñ€Ð¸ÑÐºÐ¸ Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ';
    } else {
      mockContent = 'ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ð“Ð°Ð»Ð¸Ð½Ð°, Ð²Ð°Ñˆ AI-ÑŽÑ€Ð¸ÑÑ‚. Ð¯ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ð²Ð°Ð¼ Ñ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸. Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð¼Ð½Ðµ Ð»ÑŽÐ±Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¤ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ð¸.';
    }

    // Create a mock streaming response
    const mockStream = new ReadableStream({
      start(controller) {
        const words = mockContent.split(' ');
        let currentContent = '';

        const sendChunk = (index) => {
          if (index >= words.length) {
            // Send final chunk
            controller.enqueue(`data: [DONE]\n\n`);
            controller.close();
            return;
          }

          currentContent += (index > 0 ? ' ' : '') + words[index];
          const chunk = {
            id: `chatcmpl-${Date.now()}`,
            object: 'chat.completion.chunk',
            created: Math.floor(Date.now() / 1000),
            model: model,
            choices: [{
              index: 0,
              delta: {
                content: (index > 0 ? ' ' : '') + words[index]
              },
              finish_reason: index === words.length - 1 ? 'stop' : null
            }]
          };

          controller.enqueue(`data: ${JSON.stringify(chunk)}\n\n`);

          setTimeout(() => sendChunk(index + 1), 100);
        };

        sendChunk(0);
      }
    });

    return new Response(mockStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
      },
    });
  }
}

module.exports = new ChatService();
