import { useState, useRef, useEffect, useCallback } from 'react'
import { VOICE_CONFIG, SPEECH_CONFIG, BEEP_CONFIG } from '@/config/constants'
import { transcribeAudioWithWhisper } from '@/utils/apiUtils'

interface UseVoiceOptions {
  onTranscript?: (transcript: string) => void
  onInterimTranscript?: (transcript: string) => void
  onError?: (error: string) => void
  onInfo?: (message: string) => void // –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
  onStart?: () => void
  onEnd?: () => void
  onSpeakingStart?: () => void
  onSpeakingEnd?: () => void
  useWhisperFallback?: boolean // –í–∫–ª—é—á–∏—Ç—å fallback —á–µ—Ä–µ–∑ OpenAI Whisper
  silenceTimeout?: number // –¢–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è –≤ ms (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3000)
}

export const useVoice = (options: UseVoiceOptions = {}) => {
  const { onTranscript, onInterimTranscript, onError, onInfo, onStart, onEnd, onSpeakingStart, onSpeakingEnd, useWhisperFallback = true, silenceTimeout = 3000 } = options

  const [isListening, setIsListening] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [isSupported, setIsSupported] = useState(false)

  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const synthRef = useRef<SpeechSynthesis | null>(null)
  const beepIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const sessionEndedRef = useRef(false)
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const lastResultTimeRef = useRef<number>(0)

  // –î–ª—è fallback —á–µ—Ä–µ–∑ OpenAI Whisper
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const isUsingFallbackRef = useRef(false)

  // Check if speech recognition is supported
  useEffect(() => {
    const SpeechRecognition =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition

    setIsSupported(!!SpeechRecognition)

    if (SpeechRecognition) {
      recognitionRef.current = new SpeechRecognition()
      const recognition = recognitionRef.current

      recognition.continuous = VOICE_CONFIG.CONTINUOUS
      recognition.interimResults = VOICE_CONFIG.INTERIM_RESULTS
      recognition.lang = VOICE_CONFIG.LANG
      recognition.maxAlternatives = VOICE_CONFIG.MAX_ALTERNATIVES
      
      // –í–∞–∂–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
      try {
        // @ts-ignore - WebKit specific properties
        if (recognition.speechRecognitionList) {
          recognition.speechRecognitionList = null;
        }
      } catch (e) {
        console.log('Could not set WebKit specific properties');
      }

      recognition.onstart = () => {
        console.log('üé§ Speech recognition started successfully')
        setIsListening(true)
        sessionEndedRef.current = false
        lastResultTimeRef.current = Date.now()
        onStart?.()
        startBeepInterval()
        resetSilenceTimeout() // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è
        onInfo?.('üé§ –ú–∏–∫—Ä–æ—Ñ–æ–Ω –∞–∫—Ç–∏–≤–µ–Ω! –ì–æ–≤–æ—Ä–∏—Ç–µ —Å–µ–π—á–∞—Å...')
      }

      recognition.onspeechstart = () => {
        console.log('üé§ Speech detected! User started speaking')
        resetSilenceTimeout() // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è –ø—Ä–∏ –Ω–∞—á–∞–ª–µ —Ä–µ—á–∏
      }

      recognition.onspeechend = () => {
        console.log('Speech ended - user stopped speaking')
      }

      recognition.onaudiostart = () => {
        console.log('Audio capture started')
      }

      recognition.onaudioend = () => {
        console.log('Audio capture ended')
      }

      recognition.onsoundstart = () => {
        console.log('Sound detected')
      }

      recognition.onsoundend = () => {
        console.log('Sound ended')
      }

      recognition.onresult = (event) => {
        console.log('üé§ Recognition result received:', event.results.length, 'results')
        console.log('üé§ Result index:', event.resultIndex)
        let finalTranscript = ''
        let interimTranscript = ''

        // –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i]
          const transcript = result[0].transcript
          const confidence = result[0].confidence

          console.log(`üé§ Result ${i}: "${transcript}", isFinal: ${result.isFinal}, confidence: ${confidence}`)

          if (result.isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }

        // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        if (interimTranscript) {
          console.log('üé§ Interim transcript:', interimTranscript)
          onInterimTranscript?.(interimTranscript)
          setTranscript(interimTranscript)
        }

        // –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if (finalTranscript.trim()) {
          console.log('‚úÖ Final transcript received:', finalTranscript)
          setTranscript(finalTranscript.trim())
          onTranscript?.(finalTranscript.trim())

          // –î–ª—è continuous: false –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
          setTimeout(() => {
            if (isListening && recognitionRef.current && !sessionEndedRef.current) {
              try {
                console.log('üîÑ Restarting recognition after final result')
                recognitionRef.current.start()
              } catch (error) {
                console.error('‚ùå Failed to restart recognition:', error)
              }
            }
          }, 1000)
        }

        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è –ø—Ä–∏ –ª—é–±–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        resetSilenceTimeout()
      }

      recognition.onerror = async (event) => {
        console.log('Speech recognition event:', event.error, event.message)
        setIsListening(false)
        stopBeepInterval()
        clearSilenceTimeout() // –û—á–∏—â–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
        if (event.error === 'aborted') {
          console.log('Speech recognition was aborted (normal behavior)')
          // –î–ª—è aborted –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        } else if (event.error === 'no-speech') {
          console.log('üé§ No speech detected - trying Whisper fallback if enabled')

          // –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω fallback —á–µ—Ä–µ–∑ Whisper, –ø—Ä–æ–±—É–µ–º –∑–∞–ø–∏—Å–∞—Ç—å –∞—É–¥–∏–æ
          if (useWhisperFallback && !isUsingFallbackRef.current) {
            const hasApiKey = import.meta.env.VITE_OPENAI_API_KEY && import.meta.env.VITE_OPENAI_API_KEY.length > 10
            if (!hasApiKey) {
              console.log('‚ùå OpenAI API key not found, skipping Whisper fallback')
              onError?.('API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.')
              return
            }

            console.log('üéµ Switching to Whisper fallback...')
            onInfo?.('üé§ –†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: –≥–æ–≤–æ—Ä–∏—Ç–µ —á–µ—Ç–∫–æ –≤ –º–∏–∫—Ä–æ—Ñ–æ–Ω 5 —Å–µ–∫—É–Ω–¥...')
            isUsingFallbackRef.current = true
            await startWhisperFallback()
          } else {
            // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            onError?.('‚ùå –†–µ—á—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n‚Ä¢ –ì–æ–≤–æ—Ä–∏—Ç–µ –≥—Ä–æ–º—á–µ –∏ –±–ª–∏–∂–µ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É\n‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω\n‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –±—Ä–∞—É–∑–µ—Ä (Chrome)\n‚Ä¢ –û–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É')
          }
        } else if (event.error === 'audio-capture') {
          console.error('Audio capture error - microphone issue')
          onError?.('–û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ –∑–≤—É–∫–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.')
        } else if (event.error === 'not-allowed') {
          console.error('Microphone access denied')
          onError?.('–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.')
        } else if (event.error === 'network') {
          console.error('Network error during speech recognition')
          onError?.('–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏.')
        } else {
          console.error('Speech recognition error:', event.error)
          onError?.(`–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: ${event.error}`)
        }

        if (!sessionEndedRef.current) {
          sessionEndedRef.current = true
          onEnd?.()
        }
      }

      recognition.onend = () => {
        console.log('Speech recognition ended normally')
        setIsListening(false)
        stopBeepInterval()
        clearSilenceTimeout() // –û—á–∏—â–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è

        if (!sessionEndedRef.current) {
          sessionEndedRef.current = true
          onEnd?.()
        }
      }
    }

    // Initialize speech synthesis
    if ('speechSynthesis' in window) {
      synthRef.current = window.speechSynthesis
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      if (synthRef.current) {
        synthRef.current.cancel()
      }
      stopBeepInterval()
      clearSilenceTimeout()
    }
  }, [onTranscript, onError, onStart, onEnd])

  // Play beep sound
  const playBeep = useCallback(() => {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
    const oscillator = audioContext.createOscillator()
    const gainNode = audioContext.createGain()

    oscillator.connect(gainNode)
    gainNode.connect(audioContext.destination)

    oscillator.frequency.value = BEEP_CONFIG.FREQUENCY
    oscillator.type = BEEP_CONFIG.TYPE

    gainNode.gain.setValueAtTime(0.3, audioContext.currentTime)
    gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + BEEP_CONFIG.DURATION)

    oscillator.start(audioContext.currentTime)
    oscillator.stop(audioContext.currentTime + BEEP_CONFIG.DURATION)
  }, [])

  // Start beep interval
  const startBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current)
    }
    beepIntervalRef.current = setInterval(playBeep, BEEP_CONFIG.INTERVAL)
  }, [playBeep])

  // Stop beep interval
  const stopBeepInterval = useCallback(() => {
    if (beepIntervalRef.current) {
      clearInterval(beepIntervalRef.current)
      beepIntervalRef.current = null
    }
  }, [])

  // Reset silence timeout
  const resetSilenceTimeout = useCallback(() => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current)
    }
    silenceTimeoutRef.current = setTimeout(() => {
      console.log(`Silence timeout reached (${silenceTimeout}ms), stopping recognition`)
      if (recognitionRef.current && isListening) {
        recognitionRef.current.stop()
      }
    }, silenceTimeout)
  }, [isListening, silenceTimeout])

  // Clear silence timeout
  const clearSilenceTimeout = useCallback(() => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current)
      silenceTimeoutRef.current = null
    }
  }, [])

  // Start listening
  const startListening = useCallback(() => {
    if (recognitionRef.current && !isListening) {
      try {
        console.log('üé§ Starting speech recognition session')
        recognitionRef.current.start()
      } catch (error: any) {
        console.error('‚ùå Error starting speech recognition:', error)
        if (error.message && error.message.includes('not-allowed')) {
          onError?.('–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.')
        } else if (error.message && error.message.includes('already started')) {
          console.log('Recognition already started, ignoring')
        } else {
          onError?.('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É.')
        }
      }
    } else {
      console.log('Speech recognition already active or not available')
    }
  }, [isListening, onError])

  // Stop listening
  const stopListening = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
      clearSilenceTimeout() // –û—á–∏—â–∞–µ–º —Ç–∞–π–º–µ—Ä –º–æ–ª—á–∞–Ω–∏—è
    }

    // –¢–∞–∫–∂–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º MediaRecorder –µ—Å–ª–∏ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      console.log('üéµ Stopping MediaRecorder...')
      mediaRecorderRef.current.stop()
    }

    isUsingFallbackRef.current = false
  }, [isListening])

  // Fallback —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI Whisper
  const startWhisperFallback = useCallback(async () => {
    try {
      console.log('üéµ Starting Whisper fallback recording...')
      onStart?.() // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–∞—á–∞–ª–µ –∑–∞–ø–∏—Å–∏

      // –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      // –°–æ–∑–¥–∞–µ–º MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        console.log('üéµ Audio recording stopped, sending to Whisper...')

        // –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫
        stream.getTracks().forEach(track => track.stop())

        // –°–æ–∑–¥–∞–µ–º Blob –∏–∑ chunks
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        console.log('üéµ Audio blob created:', audioBlob.size, 'bytes')

        try {
          // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ OpenAI Whisper
          const transcription = await transcribeAudioWithWhisper(audioBlob)

          if (transcription && transcription.trim()) {
            console.log('‚úÖ Whisper fallback successful:', transcription)
            setTranscript(transcription.trim())
            onTranscript?.(transcription.trim())
          } else {
            console.log('‚ö†Ô∏è Whisper returned empty transcription')
            onError?.('–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å —á–µ—Ç—á–µ.')
          }
        } catch (whisperError) {
          console.error('‚ùå Whisper fallback failed:', whisperError)
          onError?.('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.')
        } finally {
          isUsingFallbackRef.current = false
          setIsListening(false)
          onEnd?.()
        }
      }

      mediaRecorder.onerror = (error) => {
        console.error('‚ùå MediaRecorder error:', error)
        onError?.('–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ.')
        isUsingFallbackRef.current = false
        setIsListening(false)
        onEnd?.()
      }

      // –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–ø–∏—Å—å
      mediaRecorder.start()
      console.log('üéµ Whisper fallback recording started for 5 seconds')
      setIsListening(true) // –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ

      // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥
      setTimeout(() => {
        if (mediaRecorder.state === 'recording') {
          console.log('üéµ Auto-stopping Whisper recording after 5 seconds')
          mediaRecorder.stop()
        }
      }, 5000)

    } catch (error) {
      console.error('‚ùå Failed to start Whisper fallback:', error)
      onError?.('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.')
      isUsingFallbackRef.current = false
      setIsListening(false)
      onEnd?.()
    }
  }, [onTranscript, onError, onEnd])

  // Speak text
  const speak = useCallback((text: string) => {
    if (synthRef.current) {
      setIsSpeaking(true)
      onSpeakingStart?.()

      // Cancel previous speech
      synthRef.current.cancel()

      const utterance = new SpeechSynthesisUtterance(text)
      utterance.lang = SPEECH_CONFIG.LANG
      utterance.rate = SPEECH_CONFIG.RATE
      utterance.pitch = SPEECH_CONFIG.PITCH
      utterance.volume = SPEECH_CONFIG.VOLUME

      utterance.onend = () => {
        setIsSpeaking(false)
        onSpeakingEnd?.()
      }

      utterance.onerror = (event) => {
        console.error('Speech synthesis error:', event)
        setIsSpeaking(false)
        onSpeakingEnd?.()
        onError?.('–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏')
      }

      synthRef.current.speak(utterance)
    }
  }, [onError, onSpeakingStart, onSpeakingEnd])

  // Stop speaking
  const stopSpeaking = useCallback(() => {
    if (synthRef.current) {
      synthRef.current.cancel()
      setIsSpeaking(false)
      onSpeakingEnd?.()
    }
  }, [onSpeakingEnd])

  // Clear transcript
  const clearTranscript = useCallback(() => {
    setTranscript('')
  }, [])

  return {
    isListening,
    isSpeaking,
    transcript,
    isSupported,
    startListening,
    stopListening,
    speak,
    stopSpeaking,
    clearTranscript,
  }
}
