# üîå –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π LLM —Å–∏—Å—Ç–µ–º—ã

–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM –≤ –≤–∞—à–∏ –ø—Ä–æ–µ–∫—Ç—ã.

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è](#–±–∞–∑–æ–≤–∞—è-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
2. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-–≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏)
3. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —á–∞—Ç–±–æ—Ç–∞–º–∏](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-—á–∞—Ç–±–æ—Ç–∞–º–∏)
4. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è-—Å-—Å–∏—Å—Ç–µ–º–∞–º–∏-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)
5. [–ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö LLM](#–ø—Ä–∏–º–µ—Ä—ã-–¥–ª—è-–ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö-llm)
6. [–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ-–∏-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)

## üöÄ –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–¥

```python
from llm_api import quick_process

# –í–∞—à LLM –æ—Ç–≤–µ—Ç
response = get_llm_response(user_query)

# –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
result = quick_process(response)

# –í—ã–≤–µ—Å—Ç–∏
print(result.markdown)
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: –° –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

```python
from llm_api import LLMAPIClient

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–¥–∏–Ω —Ä–∞–∑
client = LLMAPIClient(
    max_length=3000,
    similarity_threshold=0.70,
    verbose=False
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è
result = client.process(response_text, title="My Document")
return result.markdown
```

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å

```python
from professional_llm_system import (
    ProfessionalLLMProcessor,
    ParagraphAnalyzer,
    DuplicateDetector
)

# –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
processor = ProfessionalLLMProcessor()
processor.paragraph_analyzer = ParagraphAnalyzer()
processor.duplicate_detector = DuplicateDetector(similarity_threshold=0.80)

# –û–±—Ä–∞–±–æ—Ç–∞—Ç—å
llm_response = processor.process(text, title="Document")

# –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
output = processor.get_markdown(llm_response)
```

## üåê –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏

### Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

```python
from flask import Flask, request, jsonify
from llm_api import LLMAPIClient

app = Flask(__name__)
llm_client = LLMAPIClient(verbose=False)

@app.route('/api/process', methods=['POST'])
def process_llm_response():
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å JSON"""
    data = request.json
    text = data.get('text', '')
    title = data.get('title', '')
    
    try:
        result = llm_client.process(text, title)
        
        return jsonify({
            'success': True,
            'markdown': result.markdown,
            'summary': result.summary,
            'statistics': result.statistics,
            'json': result.json
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/api/process/markdown', methods=['POST'])
def process_as_markdown():
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ Markdown"""
    data = request.json
    text = data.get('text', '')
    
    result = llm_client.process(text)
    return jsonify({'markdown': result.markdown})

@app.route('/api/process/batch', methods=['POST'])
def process_batch():
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤"""
    data = request.json
    responses = data.get('responses', [])
    
    results = llm_client.process_batch(responses)
    
    return jsonify({
        'count': len(results),
        'results': [
            {
                'markdown': r.markdown,
                'statistics': r.statistics
            }
            for r in results
        ]
    })

if __name__ == '__main__':
    app.run(debug=True)
```

### Django –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

```python
# views.py
from django.http import JsonResponse
from django.views.decorators.http import require_POST
from django.middleware.csrf import csrf_exempt
from llm_api import LLMAPIClient
import json

llm_client = LLMAPIClient()

@csrf_exempt
@require_POST
def process_response(request):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç"""
    try:
        data = json.loads(request.body)
        text = data.get('text', '')
        title = data.get('title', '')
        
        result = llm_client.process(text, title)
        
        return JsonResponse({
            'success': True,
            'markdown': result.markdown,
            'json': result.json,
            'statistics': result.statistics
        })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        }, status=400)

# urls.py
from django.urls import path
from . import views

urlpatterns = [
    path('api/process/', views.process_response, name='process_response'),
]
```

### FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from llm_api import LLMAPIClient
from typing import List, Optional

app = FastAPI()
llm_client = LLMAPIClient()

class ProcessRequest(BaseModel):
    text: str
    title: Optional[str] = ""

class ProcessBatchRequest(BaseModel):
    responses: List[dict]

@app.post("/api/process")
async def process(request: ProcessRequest):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç"""
    try:
        result = llm_client.process(request.text, request.title)
        
        return {
            "success": True,
            "markdown": result.markdown,
            "json": result.json,
            "statistics": result.statistics,
            "summary": result.summary
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/process/batch")
async def process_batch(request: ProcessBatchRequest):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤"""
    try:
        results = llm_client.process_batch(request.responses)
        
        return {
            "success": True,
            "count": len(results),
            "results": [
                {
                    "markdown": r.markdown,
                    "statistics": r.statistics
                }
                for r in results
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
```

## ü§ñ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —á–∞—Ç–±–æ—Ç–∞–º–∏

### –¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç (python-telegram-bot)

```python
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from llm_api import quick_process

class LLMBot:
    def __init__(self, token: str):
        self.app = Application.builder().token(token).build()
        self.setup_handlers()
    
    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        await update.message.reply_text(
            "–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ LLM –æ—Ç–≤–µ—Ç –∏ —è –µ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é.\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π /help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏."
        )
    
    async def process_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç"""
        text = update.message.text
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        await update.message.chat.send_action("typing")
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å
            result = quick_process(text)
            
            # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            response = f"**‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ**\n\n"
            response += f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
            response += f"‚Ä¢ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {result.statistics['original_length']} —Å–∏–º–≤–æ–ª–æ–≤\n"
            response += f"‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: {result.statistics['optimized_length']} —Å–∏–º–≤–æ–ª–æ–≤\n"
            response += f"‚Ä¢ –°–∂–∞—Ç–∏–µ: {result.statistics['compression_ratio']:.1%}\n"
            response += f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: {result.statistics['quality_score']:.1%}\n"
            response += f"\n**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n"
            response += result.summary[:500] + "..."
            
            await update.message.reply_text(response, parse_mode="Markdown")
            
            # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            # await update.message.reply_document(...)
            
        except Exception as e:
            await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
    
    def setup_handlers(self):
        self.app.add_handler(CommandHandler("start", self.start))
        self.app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.process_message))
    
    def run(self):
        self.app.run_polling()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    bot = LLMBot("YOUR_TELEGRAM_TOKEN")
    bot.run()
```

### Discord-–±–æ—Ç

```python
import discord
from discord.ext import commands
from llm_api import quick_process

class LLMBot(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
    
    @commands.Cog.listener()
    async def on_ready(self):
        print(f'{self.bot.user} has connected to Discord!')
    
    @commands.command(name='process', help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç')
    async def process_llm(self, ctx, *, text=None):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç"""
        if not text:
            await ctx.send("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        async with ctx.typing():
            try:
                result = quick_process(text, title=ctx.author.name)
                
                # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                embed = discord.Embed(
                    title="‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ",
                    description=result.summary[:1500],
                    color=discord.Color.green()
                )
                
                # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—è —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
                stats = result.statistics
                embed.add_field(
                    name="üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
                    value=f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {stats['original_length']}\n"
                          f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: {stats['optimized_length']}\n"
                          f"–°–∂–∞—Ç–∏–µ: {stats['compression_ratio']:.1%}\n"
                          f"–ö–∞—á–µ—Å—Ç–≤–æ: {stats['quality_score']:.1%}",
                    inline=False
                )
                
                await ctx.send(embed=embed)
                
            except Exception as e:
                await ctx.send(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞
intents = discord.Intents.default()
intents.message_content = True

bot = commands.Bot(command_prefix='!', intents=intents)

async def setup():
    await bot.add_cog(LLMBot(bot))

bot.setup_hook = setup
bot.run("YOUR_DISCORD_TOKEN")
```

## üìö –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

### MkDocs –ø–ª–∞–≥–∏–Ω

```python
# plugins/llm_processor.py
from mkdocs.plugins import BasePlugin
from mkdocs.config import config_options
from llm_api import quick_process

class LLMProcessorPlugin(BasePlugin):
    config_scheme = (
        ('max_length', config_options.Type(int, default=2000)),
        ('similarity_threshold', config_options.Type(float, default=0.70)),
        ('enabled_tags', config_options.Type(list, default=['llm-process'])),
    )
    
    def on_page_markdown(self, markdown, page, config, files):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å markdown —Å LLM –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º"""
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–≥–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if not any(tag in markdown for tag in self.config['enabled_tags']):
            return markdown
        
        # –ù–∞–π—Ç–∏ LLM –±–ª–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–µ–∂–¥—É <!-- llm-process --> –∏ <!-- /llm-process -->)
        import re
        
        pattern = r'<!-- llm-process -->(.*?)<!-- /llm-process -->'
        matches = re.finditer(pattern, markdown, re.DOTALL)
        
        processed = markdown
        for match in matches:
            content = match.group(1).strip()
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
            result = quick_process(content)
            
            # –ó–∞–º–µ–Ω–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            processed = processed.replace(match.group(0), result.markdown)
        
        return processed

# mkdocs.yml
plugins:
  - search
  - llm_processor:
      max_length: 3000
      similarity_threshold: 0.75
```

### Sphinx —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ

```python
# source/conf.py
from llm_api import quick_process

# –í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
extensions = [
    'sphinx.ext.autodoc',
    'llm_processor'
]

# source/llm_processor.py
from docutils import nodes
from docutils.parsers.rst import Directive
from llm_api import quick_process

class LLMProcessDirective(Directive):
    has_content = True
    
    def run(self):
        text = '\n'.join(self.content)
        result = quick_process(text)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å markdown –≤ RST –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        output = result.markdown
        
        raw_node = nodes.raw('', output, format='html')
        return [raw_node]

def setup(app):
    app.add_directive("llm-process", LLMProcessDirective)
    return {
        'version': '1.0',
        'parallel_read_safe': True,
    }
```

## üîó –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö LLM

### OpenAI ChatGPT

```python
import openai
from llm_api import LLMIntegration

async def process_gpt_response(user_query: str) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç ChatGPT"""
    
    # –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç OpenAI
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_query}
        ]
    )
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
    result = LLMIntegration.process_openai_response(response)
    
    return result.markdown
```

### Anthropic Claude

```python
import anthropic
from llm_api import LLMIntegration

def process_claude_response(user_query: str) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Claude"""
    
    client = anthropic.Anthropic()
    
    message = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[
            {"role": "user", "content": user_query}
        ]
    )
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
    result = LLMIntegration.process_anthropic_response({
        'content': message.content
    })
    
    return result.markdown
```

### Google Gemini

```python
import google.generativeai as genai
from llm_api import quick_process

def process_gemini_response(user_query: str) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Gemini"""
    
    genai.configure(api_key="YOUR_API_KEY")
    model = genai.GenerativeModel('gemini-pro')
    
    response = model.generate_content(user_query)
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
    result = quick_process(response.text)
    
    return result.markdown
```

### Open Source LLMs (Ollama, LLaMA, –∏ —Ç.–¥.)

```python
import requests
from llm_api import quick_process

def process_local_llm(user_query: str, model_name: str = "mistral") -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π LLM"""
    
    # –ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ Ollama)
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model_name,
            "prompt": user_query,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        text = response.json()['response']
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
        result = quick_process(text)
        return result.markdown
    else:
        raise Exception(f"Error: {response.status_code}")
```

## ‚öôÔ∏è –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
from functools import lru_cache
from llm_api import LLMAPIClient
import hashlib

class CachedLLMClient:
    def __init__(self):
        self.client = LLMAPIClient()
        self.cache = {}
    
    def process(self, text: str, title: str = "") -> str:
        """–ü—Ä–æ—Ü–µ—Å—Å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        # –°–æ–∑–¥–∞—Ç—å –∫–ª—é—á –∫—ç—à–∞
        cache_key = hashlib.md5(f"{text}:{title}".encode()).hexdigest()
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å
        result = self.client.process(text, title)
        output = result.markdown
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
        self.cache[cache_key] = output
        
        return output
    
    def clear_cache(self):
        self.cache.clear()
```

### –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
import asyncio
from llm_api import LLMAPIClient
from concurrent.futures import ThreadPoolExecutor

class ParallelLLMClient:
    def __init__(self, max_workers: int = 4):
        self.client = LLMAPIClient()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_parallel(self, texts: list) -> list:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        
        loop = asyncio.get_event_loop()
        
        tasks = [
            loop.run_in_executor(
                self.executor,
                self.client.process,
                text
            )
            for text in texts
        ]
        
        results = await asyncio.gather(*tasks)
        return results
    
    async def process_batch_parallel(self, batch_requests: list) -> list:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞–∫–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        
        loop = asyncio.get_event_loop()
        
        async def process_item(item):
            return await loop.run_in_executor(
                self.executor,
                lambda: self.client.process(
                    item.get('content'),
                    item.get('title', '')
                )
            )
        
        results = await asyncio.gather(*[
            process_item(item) for item in batch_requests
        ])
        
        return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    client = ParallelLLMClient()
    
    texts = ["Text 1", "Text 2", "Text 3"]
    results = await client.process_parallel(texts)
    
    for result in results:
        print(result.markdown)

asyncio.run(main())
```

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã

### 1. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤

```python
# –î–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π (–º–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
news_client = LLMAPIClient(
    similarity_threshold=0.65,  # –ë–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ
    max_length=2000
)

# –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
tech_client = LLMAPIClient(
    similarity_threshold=0.75,
    max_length=3000
)

# –î–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
legal_client = LLMAPIClient(
    similarity_threshold=0.80,  # –ú–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ
    max_length=5000
)
```

### 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
from llm_api import quick_process
import logging

logger = logging.getLogger(__name__)

def safe_process(text: str, title: str = "") -> dict:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    try:
        result = quick_process(text, title)
        return {
            'success': True,
            'markdown': result.markdown,
            'statistics': result.statistics
        }
    except ValueError as e:
        logger.error(f"Value error: {e}")
        return {'success': False, 'error': f'Invalid input: {e}'}
    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
        return {'success': False, 'error': 'Processing failed'}
```

### 3. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```python
import time
from llm_api import LLMAPIClient

class MonitoredLLMClient:
    def __init__(self):
        self.client = LLMAPIClient()
        self.stats = {
            'processed': 0,
            'total_time': 0,
            'errors': 0
        }
    
    def process(self, text: str, title: str = ""):
        start = time.time()
        
        try:
            result = self.client.process(text, title)
            self.stats['processed'] += 1
            self.stats['total_time'] += time.time() - start
            
            print(f"‚úÖ Processed in {time.time() - start:.2f}s")
            print(f"   Compression: {result.statistics['compression_ratio']:.1%}")
            
            return result
        except Exception as e:
            self.stats['errors'] += 1
            print(f"‚ùå Error: {e}")
            raise
    
    def print_stats(self):
        avg_time = self.stats['total_time'] / self.stats['processed'] if self.stats['processed'] > 0 else 0
        print(f"üìä Statistics:")
        print(f"   Processed: {self.stats['processed']}")
        print(f"   Avg time: {avg_time:.2f}s")
        print(f"   Errors: {self.stats['errors']}")
```

---

**–°—á–∞—Å—Ç–ª–∏–≤–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏!** üöÄ

