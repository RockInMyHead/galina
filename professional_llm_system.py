"""
üéØ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –†–ê–ë–û–¢–´ –° LLM
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤
"""

import re
import json
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from enum import Enum
from abc import ABC, abstractmethod
from collections import defaultdict, Counter
import time

# ============================================================================
# 1. –¢–ò–ü–´ –ò –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–•
# ============================================================================

class ContentType(Enum):
    """–¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ LLM"""
    DEFINITION = "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
    INSTRUCTION = "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"
    WARNING = "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ"
    EXAMPLE = "–ø—Ä–∏–º–µ—Ä"
    SUMMARY = "—Ä–µ–∑—é–º–µ"
    RISK = "—Ä–∏—Å–∫"


@dataclass
class ContentChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–ª–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    type: ContentType
    title: str
    content: str
    importance: float = 0.5  # 0-1, –≤–∞–∂–Ω–æ—Å—Ç—å –±–ª–æ–∫–∞
    duplicated_from: Optional[int] = None  # –∏–Ω–¥–µ–∫—Å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞ –µ—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç
    similarity_score: float = 0.0

    def to_dict(self):
        return {
            'type': self.type.value,
            'title': self.title,
            'content': self.content,
            'importance': self.importance,
            'duplicated_from': self.duplicated_from,
            'similarity_score': self.similarity_score
        }


@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
    original_text: str
    chunks: List[ContentChunk]
    metadata: Dict = None
    processing_time: float = 0.0
    deduplication_ratio: float = 0.0
    quality_score: float = 0.0

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


# ============================================================================
# 2. –ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ –ö–û–ù–¢–ï–ù–¢–ê
# ============================================================================

class ContentAnalyzer(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    @abstractmethod
    def analyze(self, text: str) -> List[ContentChunk]:
        pass


class SectionAnalyzer(ContentAnalyzer):
    """–ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º"""
    
    def analyze(self, text: str) -> List[ContentChunk]:
        chunks = []
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        sections = re.split(r'\n(\d+)\.\s+', text)
        
        for i in range(1, len(sections), 2):
            if i + 1 < len(sections):
                section_num = sections[i]
                section_title = sections[i + 1].split('\n')[0].strip()
                section_content = '\n'.join(sections[i + 1].split('\n')[1:])
                
                chunk = ContentChunk(
                    type=ContentType.DEFINITION,
                    title=f"–†–∞–∑–¥–µ–ª {section_num}: {section_title}",
                    content=section_content.strip(),
                    importance=0.8
                )
                chunks.append(chunk)
        
        return chunks


class ParagraphAnalyzer(ContentAnalyzer):
    """–ê–Ω–∞–ª–∏–∑ –ø–æ –∞–±–∑–∞—Ü–∞–º –∏ –∏—Ö —Ç–∏–ø–∞–º"""
    
    def __init__(self):
        self.patterns = {
            ContentType.WARNING: r'(–í–Ω–∏–º–∞–Ω–∏–µ|–í–∞–∂–Ω–æ|–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ|–†–∏—Å–∫)[:!]',
            ContentType.INSTRUCTION: r'(–ö–∞–∫|–ß—Ç–æ–±—ã|–°–ª–µ–¥—É–µ—Ç|–ù—É–∂–Ω–æ|–ù–µ–æ–±—Ö–æ–¥–∏–º–æ)',
            ContentType.EXAMPLE: r'(–ù–∞–ø—Ä–∏–º–µ—Ä|–ü—Ä–∏–º–µ—Ä)',
            ContentType.SUMMARY: r'(–ò—Ç–æ–≥–æ|–í –∏—Ç–æ–≥–µ|–ó–∞–∫–ª—é—á–µ–Ω–∏–µ|–í—ã–≤–æ–¥)',
        }
    
    def analyze(self, text: str) -> List[ContentChunk]:
        chunks = []
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            para = para.strip()
            if not para or len(para) < 10:
                continue
            
            content_type = self._determine_type(para)
            title = self._extract_title(para)
            
            chunk = ContentChunk(
                type=content_type,
                title=title or "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                content=para,
                importance=self._calculate_importance(content_type)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _determine_type(self, text: str) -> ContentType:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text_lower = text.lower()
        
        for content_type, pattern in self.patterns.items():
            if re.search(pattern, text_lower):
                return content_type
        
        return ContentType.DEFINITION
    
    def _extract_title(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        first_line = text.split('\n')[0]
        
        # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è –ª–∏–Ω–∏—è –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–≤–æ–µ—Ç–æ—á–∏–µ–º –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–æ–º
        if first_line.endswith(':') or first_line.endswith('?'):
            return first_line[:-1].strip()
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç—á–µ 100 —Å–∏–º–≤–æ–ª–æ–≤, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if len(first_line) < 100 and not first_line.endswith('.'):
            return first_line
        
        return None
    
    def _calculate_importance(self, content_type: ContentType) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø—É"""
        importance_map = {
            ContentType.WARNING: 1.0,
            ContentType.INSTRUCTION: 0.9,
            ContentType.SUMMARY: 0.85,
            ContentType.EXAMPLE: 0.5,
            ContentType.DEFINITION: 0.7,
            ContentType.RISK: 0.95,
        }
        return importance_map.get(content_type, 0.5)


# ============================================================================
# 3. –î–ï–¢–ï–ö–¢–û–† –î–£–ë–õ–ò–ö–ê–¢–û–í
# ============================================================================

class DuplicateDetector:
    """–£–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, similarity_threshold: float = 0.75):
        self.similarity_threshold = similarity_threshold
        self.keyword_cache = {}
    
    def detect_duplicates(self, chunks: List[ContentChunk]) -> List[ContentChunk]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∏ –æ—Ç–º–µ—Ç–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —á–∞–Ω–∫–∞—Ö"""
        processed_chunks = []
        
        for i, chunk in enumerate(chunks):
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            if len(chunk.content) < 20:
                processed_chunks.append(chunk)
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
            max_similarity = 0.0
            similar_index = None
            
            for j in range(i):
                if chunks[j].duplicated_from is not None:
                    continue  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
                
                similarity = self._calculate_similarity(
                    chunk.content,
                    chunks[j].content
                )
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    similar_index = j
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –ø–æ—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            if max_similarity >= self.similarity_threshold:
                chunk.duplicated_from = similar_index
                chunk.similarity_score = max_similarity
            
            processed_chunks.append(chunk)
        
        return processed_chunks
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ (0-1)"""
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
        # 1. –°—Ö–æ–∂–µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords1 = set(self._extract_keywords(text1))
        keywords2 = set(self._extract_keywords(text2))
        
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)
        keyword_similarity = intersection / union if union > 0 else 0
        
        # 2. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (–¥–ª–∏–Ω–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
        len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))
        
        # 3. –°—Ö–æ–∂–µ—Å—Ç—å –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤ (—á–∞—Å—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ)
        start_similarity = self._sequence_similarity(text1[:100], text2[:100])
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        total_similarity = (
            keyword_similarity * 0.5 +
            len_ratio * 0.2 +
            start_similarity * 0.3
        )
        
        return total_similarity
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        words = re.findall(r'\b\w{3,}\b', text.lower())
        
        # –£–±—Ä–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stopwords = {
            '—ç—Ç–æ', '—ç—Ç–æ', '–µ—Å–ª–∏', '–º–æ–∂–µ—Ç', '–Ω—É–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '—á—Ç–æ', '–∫–∞–∫',
            '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã–π', '–≤—Å–µ', '–≤—Å—ë', '–±—É–¥–µ—Ç',
            '–¥–∞–≤–∞–π—Ç–µ', '—Ä–∞–∑–±–µ—Ä–µ–º—Å—è', '–¥–∞–≤–∞–π—Ç–µ', '—Ä–∞–∑–±–µ—Ä—ë–º', '–¥–ª—è', '—Ç–æ–≥–æ',
        }
        
        return [w for w in words if w not in stopwords]
    
    def _sequence_similarity(self, s1: str, s2: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (Jaro-Winkler-–ø–æ–¥–æ–±–Ω–æ–µ)"""
        if not s1 or not s2:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        matches = sum(1 for a, b in zip(s1, s2) if a == b)
        max_len = max(len(s1), len(s2))
        
        return matches / max_len if max_len > 0 else 0


# ============================================================================
# 4. –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –ò –°–¢–†–£–ö–¢–£–†–ò–ó–ê–¢–û–†
# ============================================================================

class ResponseOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
    
    def __init__(self, max_length: int = 2000, min_importance: float = 0.5):
        self.max_length = max_length
        self.min_importance = min_importance
    
    def optimize(self, chunks: List[ContentChunk]) -> List[ContentChunk]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏"""
        
        # 1. –£–±—Ä–∞—Ç—å —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
        unique_chunks = [
            chunk for chunk in chunks
            if chunk.duplicated_from is None
        ]
        
        # 2. –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø—É –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        unique_chunks.sort(
            key=lambda x: (-x.importance, x.type.value)
        )
        
        # 3. –û–±—Ä–µ–∑–∞—Ç—å –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        if self._calculate_total_length(unique_chunks) > self.max_length:
            unique_chunks = self._truncate_smartly(unique_chunks)
        
        # 4. –ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        unique_chunks = self._regroup_by_type(unique_chunks)
        
        return unique_chunks
    
    def _calculate_total_length(self, chunks: List[ContentChunk]) -> int:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ–±—â—É—é –¥–ª–∏–Ω—É –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤"""
        return sum(len(chunk.content) for chunk in chunks)
    
    def _truncate_smartly(self, chunks: List[ContentChunk]) -> List[ContentChunk]:
        """–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω–æ–≥–æ"""
        result = []
        current_length = 0
        
        for chunk in chunks:
            chunk_length = len(chunk.content)
            
            if current_length + chunk_length <= self.max_length:
                result.append(chunk)
                current_length += chunk_length
            elif current_length < self.max_length * 0.8:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ, –¥–æ–±–∞–≤–∏—Ç—å —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                remaining = self.max_length - current_length
                if remaining > 100:
                    shortened = chunk.content[:remaining-20] + "..."
                    chunk_copy = ContentChunk(
                        type=chunk.type,
                        title=chunk.title,
                        content=shortened,
                        importance=chunk.importance
                    )
                    result.append(chunk_copy)
                break
            else:
                break
        
        return result
    
    def _regroup_by_type(self, chunks: List[ContentChunk]) -> List[ContentChunk]:
        """–ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏ –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –ª—É—á—à–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        grouped = defaultdict(list)
        
        for chunk in chunks:
            grouped[chunk.type].append(chunk)
        
        # –ü–æ—Ä—è–¥–æ–∫ —Ç–∏–ø–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞
        type_order = [
            ContentType.DEFINITION,
            ContentType.INSTRUCTION,
            ContentType.SUMMARY,
            ContentType.EXAMPLE,
            ContentType.RISK,
            ContentType.WARNING,
        ]
        
        result = []
        for content_type in type_order:
            result.extend(grouped.get(content_type, []))
        
        return result


# ============================================================================
# 5. –ì–ï–ù–ï–†–ê–¢–û–† –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–û–ì–û –í–´–í–û–î–ê
# ============================================================================

class ProfessionalOutputGenerator:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥"""
    
    def generate_markdown(self, chunks: List[ContentChunk], title: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Markdown-–≤—ã–≤–æ–¥"""
        lines = []
        
        if title:
            lines.append(f"# {title}\n")
        
        current_type = None
        section_count = defaultdict(int)
        
        for chunk in chunks:
            # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏ –µ—Å–ª–∏ —Ç–∏–ø –∏–∑–º–µ–Ω–∏–ª—Å—è
            if chunk.type != current_type:
                current_type = chunk.type
                
                type_names = {
                    ContentType.DEFINITION: "üìã –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø",
                    ContentType.INSTRUCTION: "üìù –ò–ù–°–¢–†–£–ö–¶–ò–ò",
                    ContentType.SUMMARY: "‚úÖ –†–ï–ó–Æ–ú–ï",
                    ContentType.EXAMPLE: "üí° –ü–†–ò–ú–ï–†–´",
                    ContentType.RISK: "‚ö†Ô∏è –†–ò–°–ö–ò",
                    ContentType.WARNING: "üö® –í–ù–ò–ú–ê–ù–ò–ï",
                }
                
                section_name = type_names.get(chunk.type, chunk.type.value)
                lines.append(f"\n## {section_name}\n")
            
            section_count[chunk.type] += 1
            
            # –î–æ–±–∞–≤–∏—Ç—å —á–∞–Ω–∫
            if chunk.title:
                lines.append(f"### {chunk.title}")
                lines.append("")
            
            lines.append(chunk.content)
            lines.append("")
        
        return "\n".join(lines)
    
    def generate_structured_json(self, chunks: List[ContentChunk], metadata: Dict = None) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON"""
        return {
            'metadata': metadata or {},
            'chunks_count': len(chunks),
            'chunks': [chunk.to_dict() for chunk in chunks],
            'generated_at': time.time()
        }
    
    def generate_summary(self, chunks: List[ContentChunk]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ"""
        lines = []
        
        # –°–æ–±—Ä–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã
        definition_chunks = [c for c in chunks if c.type == ContentType.DEFINITION]
        
        if definition_chunks:
            lines.append("**–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:**\n")
            for i, chunk in enumerate(definition_chunks[:5], 1):
                # –ü–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                first_sentence = chunk.content.split('.')[0] + '.'
                lines.append(f"{i}. {first_sentence}")
        
        return "\n".join(lines)


# ============================================================================
# 6. –ì–õ–ê–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–†–ê–ë–û–¢–ö–ò LLM –û–¢–í–ï–¢–û–í
# ============================================================================

class ProfessionalLLMProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.section_analyzer = SectionAnalyzer()
        self.paragraph_analyzer = ParagraphAnalyzer()
        self.duplicate_detector = DuplicateDetector(similarity_threshold=0.70)
        self.optimizer = ResponseOptimizer()
        self.output_generator = ProfessionalOutputGenerator()
    
    def process(self, llm_response: str, title: str = "") -> LLMResponse:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –æ—Ç–≤–µ—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        start_time = time.time()
        
        self._log("üîÑ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò LLM –û–¢–í–ï–¢–ê")
        self._log(f"–î–ª–∏–Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {len(llm_response)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # 1. –ê–Ω–∞–ª–∏–∑
        self._log("1Ô∏è‚É£  –ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ù–¢–ê")
        chunks = self._analyze_content(llm_response)
        self._log(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ {len(chunks)} –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        # 2. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self._log("2Ô∏è‚É£  –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í")
        chunks = self.duplicate_detector.detect_duplicates(chunks)
        duplicates_count = sum(1 for c in chunks if c.duplicated_from is not None)
        self._log(f"   ‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        if duplicates_count > 0:
            self._log("   –î—É–±–ª–∏–∫–∞—Ç—ã:")
            for i, chunk in enumerate(chunks):
                if chunk.duplicated_from is not None:
                    self._log(f"      - –ë–ª–æ–∫ {i} –¥—É–±–ª–∏—Ä—É–µ—Ç –±–ª–æ–∫ {chunk.duplicated_from} (—Å—Ö–æ–∂–µ—Å—Ç—å: {chunk.similarity_score:.1%})")
        
        # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        self._log("3Ô∏è‚É£  –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ò –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ò–ï")
        optimized_chunks = self.optimizer.optimize(chunks)
        dedup_ratio = 1 - (len(optimized_chunks) / len(chunks)) if chunks else 0
        self._log(f"   ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(optimized_chunks)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
        self._log(f"   ‚úì –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {dedup_ratio:.1%}")
        
        # 4. –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞
        self._log("4Ô∏è‚É£  –†–ê–°–ß–ï–¢ –ö–ê–ß–ï–°–¢–í–ê")
        quality_score = self._calculate_quality_score(optimized_chunks)
        self._log(f"   ‚úì –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_score:.1%}")
        
        processing_time = time.time() - start_time
        self._log(f"‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –∑–∞ {processing_time:.2f}—Å")
        
        # –°–æ–∑–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = LLMResponse(
            original_text=llm_response,
            chunks=optimized_chunks,
            processing_time=processing_time,
            deduplication_ratio=dedup_ratio,
            quality_score=quality_score,
            metadata={
                'title': title,
                'original_length': len(llm_response),
                'optimized_length': sum(len(c.content) for c in optimized_chunks),
                'chunks_removed': len(chunks) - len(optimized_chunks),
                'duplicates_found': duplicates_count,
            }
        )
        
        return result
    
    def _analyze_content(self, text: str) -> List[ContentChunk]:
        """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã
        chunks_by_section = self.section_analyzer.analyze(text)
        chunks_by_paragraph = self.paragraph_analyzer.analyze(text)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–µ–∫—Ü–∏–∏ —Å —Ö–æ—Ä–æ—à–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö
        if len(chunks_by_section) > 1:
            return chunks_by_section
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –ø–æ –∞–±–∑–∞—Ü–∞–º
        return chunks_by_paragraph
    
    def _calculate_quality_score(self, chunks: List[ContentChunk]) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        if not chunks:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å
        avg_importance = sum(c.importance for c in chunks) / len(chunks)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        type_diversity = len(set(c.type for c in chunks)) / len(ContentType)
        
        # –ù–∞–ª–∏—á–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        has_instructions = any(c.type == ContentType.INSTRUCTION for c in chunks)
        has_warnings = any(c.type == ContentType.WARNING for c in chunks)
        structure_score = 0.7 if (has_instructions and has_warnings) else 0.5
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        quality = (
            avg_importance * 0.4 +
            type_diversity * 0.3 +
            structure_score * 0.3
        )
        
        return min(quality, 1.0)
    
    def _log(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if self.verbose:
            print(message)
    
    def get_markdown(self, llm_response_obj: LLMResponse) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å Markdown-–≤—ã–≤–æ–¥"""
        return self.output_generator.generate_markdown(
            llm_response_obj.chunks,
            title=llm_response_obj.metadata.get('title', '')
        )
    
    def get_json(self, llm_response_obj: LLMResponse) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å JSON-–≤—ã–≤–æ–¥"""
        return self.output_generator.generate_structured_json(
            llm_response_obj.chunks,
            metadata=llm_response_obj.metadata
        )
    
    def get_summary(self, llm_response_obj: LLMResponse) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—é–º–µ"""
        return self.output_generator.generate_summary(llm_response_obj.chunks)


# ============================================================================
# 7. –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# ============================================================================

def demo_professional_processing():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    # LLM –æ—Ç–≤–µ—Ç (–∏–∑ –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞)
    llm_response = """–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û?

1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É—á—Ä–µ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —É—Å—Ç–∞–≤–∞ –û–û–û

–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û (–û–±—â–µ—Å—Ç–≤–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é) –∏ –∫–∞–∫ –∏—Ö –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å.

–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?

–ó–∞—è–≤–ª–µ–Ω–∏–µ –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: –≠—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å. –û–Ω–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "–§–æ—Ä–º–∞ –†11001". –í –Ω–µ–π —É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –æ–± –û–û–û, —Ç–∞–∫–∏–µ –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ, –∞–¥—Ä–µ—Å –∏ –¥–∞–Ω–Ω—ã–µ —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π.

–£—Å—Ç–∞–≤ –û–û–û: –≠—Ç–æ –≥–ª–∞–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏, –≥–¥–µ –ø—Ä–æ–ø–∏—Å–∞–Ω—ã –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞ –µ—ë —Ä–∞–±–æ—Ç—ã. –£—Å—Ç–∞–≤ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–ª—è—Ö –∫–æ–º–ø–∞–Ω–∏–∏, —Ä–∞–∑–º–µ—Ä–∞—Ö —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ –∏ –ø—Ä–∞–≤–∞—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.

–ü—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–±—Ä–∞–Ω–∏—è —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π: –ï—Å–ª–∏ –≤ –û–û–û –Ω–µ—Å–∫–æ–ª—å–∫–æ —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π, –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–æ–±—Ä–∞–Ω–∏–µ –∏ –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏. –ò—Ç–æ–≥–∏ –æ—Ñ–æ—Ä–º–ª—è—é—Ç—Å—è –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª–µ.

–ö–≤–∏—Ç–∞–Ω—Ü–∏—è –æ–± –æ–ø–ª–∞—Ç–µ –≥–æ—Å–ø–æ—à–ª–∏–Ω—ã: –ó–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –û–û–û –Ω—É–∂–Ω–æ –∑–∞–ø–ª–∞—Ç–∏—Ç—å –≥–æ—Å–ø–æ—à–ª–∏–Ω—É. –ï—ë —Ä–∞–∑–º–µ—Ä –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –Ω–∞ —Å–∞–π—Ç–µ –Ω–∞–ª–æ–≥–æ–≤–æ–π —Å–ª—É–∂–±—ã.

–ì–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –ø–∏—Å—å–º–æ –æ—Ç –≤–ª–∞–¥–µ–ª—å—Ü–∞ –ø–æ–º–µ—â–µ–Ω–∏—è: –ï—Å–ª–∏ –≤—ã –∞—Ä–µ–Ω–¥—É–µ—Ç–µ –æ—Ñ–∏—Å –∏–ª–∏ –¥—Ä—É–≥–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ, –≤–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–∏—Å—å–º–æ –æ—Ç –∞—Ä–µ–Ω–¥–æ–¥–∞—Ç–µ–ª—è, —á—Ç–æ –æ–Ω –Ω–µ –ø—Ä–æ—Ç–∏–≤ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —ç—Ç–æ–º—É –∞–¥—Ä–µ—Å—É.

–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?

–í—Å–µ —ç—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –∑–∞–∫–æ–Ω–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞—à–µ–≥–æ –±–∏–∑–Ω–µ—Å–∞ –∏ –¥–∞—é—Ç –µ–º—É –ø—Ä–∞–≤–æ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å. –ë–µ–∑ –Ω–∏—Ö –≤—ã –Ω–µ —Å–º–æ–∂–µ—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –æ—Ç–∫—Ä—ã–≤–∞—Ç—å —Å—á–µ—Ç–∞ –≤ –±–∞–Ω–∫–µ –∏–ª–∏ –∑–∞–∫–ª—é—á–∞—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã.

–ö–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã?

–ó–∞–ø–æ–ª–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É –†11001: –û–±—Ä–∞–∑–µ—Ü –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç–µ –Ω–∞–ª–æ–≥–æ–≤–æ–π —Å–ª—É–∂–±—ã. –ë—É–¥—å—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã –ø—Ä–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö.

–°–æ—Å—Ç–∞–≤—å—Ç–µ —É—Å—Ç–∞–≤: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —à–∞–±–ª–æ–Ω—ã –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —é—Ä–∏—Å—Ç—É –¥–ª—è –ø–æ–º–æ—â–∏ –≤ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –≤–∞—à–µ–≥–æ –±–∏–∑–Ω–µ—Å–∞.

–ü—Ä–æ–≤–µ–¥–∏—Ç–µ —Å–æ–±—Ä–∞–Ω–∏–µ —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π: –ï—Å–ª–∏ —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π –Ω–µ—Å–∫–æ–ª—å–∫–æ, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ–¥–∏—Ç–µ —Å–æ–±—Ä–∞–Ω–∏–µ –∏ –æ—Ñ–æ—Ä–º–∏—Ç–µ –ø—Ä–æ—Ç–æ–∫–æ–ª —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.

–û–ø–ª–∞—Ç–∏—Ç–µ –≥–æ—Å–ø–æ—à–ª–∏–Ω—É: –°–¥–µ–ª–∞—Ç—å —ç—Ç–æ –º–æ–∂–Ω–æ —á–µ—Ä–µ–∑ –±–∞–Ω–∫ –∏–ª–∏ –æ–Ω–ª–∞–π–Ω-—Å–µ—Ä–≤–∏—Å, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–≤–∏—Ç–∞–Ω—Ü–∏—é –¥–ª—è –Ω–∞–ª–æ–≥–æ–≤–æ–π.

–ü–æ–ª—É—á–∏—Ç–µ –≥–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –ø–∏—Å—å–º–æ: –ü–æ–ø—Ä–æ—Å–∏—Ç–µ –≤–ª–∞–¥–µ–ª—å—Ü–∞ –ø–æ–º–µ—â–µ–Ω–∏—è –æ—Ñ–æ—Ä–º–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∞–¥—Ä–µ—Å–∞ –∏ —Å–æ–≥–ª–∞—Å–∏—è –Ω–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –û–û–û.

2. –°–±–æ—Ä –∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∑–∞—è–≤–ª–µ–Ω–∏—è –æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏

–ö–æ–Ω–µ—á–Ω–æ, –¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û –∏ –∫–∞–∫ –∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ñ–æ—Ä–º–∏—Ç—å.

–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç

–î–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û –≤–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –∑–∞—è–≤–ª–µ–Ω–∏–µ –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏, —É—Å—Ç–∞–≤ –∫–æ–º–ø–∞–Ω–∏–∏, —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–∑–¥–∞–Ω–∏–∏ (–∏–ª–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–±—Ä–∞–Ω–∏—è —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π), –∫–≤–∏—Ç–∞–Ω—Ü–∏—è –æ–± —É–ø–ª–∞—Ç–µ –≥–æ—Å–ø–æ—à–ª–∏–Ω—ã –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥—Ä—É–≥–∏–µ.

3. –û–ø–ª–∞—Ç–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—à–ª–∏–Ω—ã –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –û–û–û

–ö–æ–Ω–µ—á–Ω–æ! –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û, –∏ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø–ª–∞—Ç–∏—Ç—å –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—É—é –ø–æ—à–ª–∏–Ω—É –∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–¥—Ä–µ—Å –≤–∞—à–µ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞.

–†–∏—Å–∫–∏

–ï—Å–ª–∏ –Ω–µ –æ–ø–ª–∞—Ç–∏—Ç—å –ø–æ—à–ª–∏–Ω—É –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–µ–∫–≤–∏–∑–∏—Ç—ã, —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞. –¢–∞–∫–∂–µ –±–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∞–¥—Ä–µ—Å–∞ –≤–∞—à—É –∫–æ–º–ø–∞–Ω–∏—é –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É—é—Ç. –ü–æ—ç—Ç–æ–º—É –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π!
"""
    
    # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = ProfessionalLLMProcessor(verbose=True)
    
    print("\n" + "="*70)
    print("üéØ –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–†–ê–ë–û–¢–ö–ò LLM –û–¢–í–ï–¢–û–í")
    print("="*70 + "\n")
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
    result = processor.process(
        llm_response,
        title="–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –û–û–û"
    )
    
    # –í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*70)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
    print("="*70)
    print(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {result.metadata['original_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {result.metadata['optimized_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤: {len(result.chunks)}")
    print(f"–°–∂–∞—Ç–∏–µ: {result.deduplication_ratio:.1%}")
    print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.2f}—Å")
    print(f"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {result.quality_score:.1%}")
    
    print("\n" + "="*70)
    print("‚ú® –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô MARKDOWN –í–´–í–û–î")
    print("="*70 + "\n")
    print(processor.get_markdown(result))
    
    print("\n" + "="*70)
    print("üìå –†–ï–ó–Æ–ú–ï")
    print("="*70 + "\n")
    print(processor.get_summary(result))
    
    print("\n" + "="*70)
    print("üíæ JSON –°–¢–†–£–ö–¢–£–†–ê")
    print("="*70 + "\n")
    json_output = processor.get_json(result)
    print(json.dumps(json_output, ensure_ascii=False, indent=2)[:500] + "...")
    
    return result


if __name__ == "__main__":
    result = demo_professional_processing()

